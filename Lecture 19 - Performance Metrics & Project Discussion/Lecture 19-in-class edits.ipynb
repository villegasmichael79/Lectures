{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 19 - Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key step in machine learning algorithm development and testing is determining a good error and evaluation metric.\n",
    "\n",
    "**Evaluation metrics** help us to estimate how well our model is trained and it is important to pick a metric that matches our overall goal for the system.\n",
    "\n",
    "Some common evaluation metrics include precision, recall, receiver operating curves, and confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Accuracy and Error\n",
    "\n",
    "Classification accuracy and e the number of correct predictions made as a ratio of all predictions made.\n",
    "\n",
    "* **Classification accuracy** is defined as the number of correctly classified samples divided by all samples:\n",
    "\n",
    "$$\\text{accuracy} = \\frac{N_{\\text{corr}}}{N}$$\n",
    "\n",
    "where $N_{\\text{corr}}$ is the number of correct classified samples and $N$ is the total number of samples.\n",
    "\n",
    "* **Classification error** is defined as the number of incorrectly classified samples divided by all samples:\n",
    "\n",
    "$$\\text{error} = \\frac{N_{\\text{miss}}}{N}$$\n",
    "\n",
    "where $N_{\\text{miss}}$ is the number of misclassified samples and $N$ is the total number of samples.\n",
    "\n",
    "* Classification accuracy is the most common evaluation metric for classification problems, it is also the most misused. It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case.\n",
    "\n",
    "## Example 1: Fish Dataset\n",
    "Suppose there is a 3-class classification problem, in which we would like to classify each training sample (a fish) to one of the three classes (A = salmon or B = sea bass or C = cod).\n",
    "\n",
    "Let's assume there are 150 samples, including 30 salmon, 40 sea bass and 80 cod. Suppose our model misclassifies 4 salmon, 2 sea bass and 5 cod.\n",
    "\n",
    "* The classification accuracy (ACC) of our binary classification model is calculated as:\n",
    "\n",
    "$$\\text{ACC} = \\frac{26 + 38 + 75}{30 + 40 + 80} = \\frac{139}{150} \\approx 92.7 \\%$$\n",
    "\n",
    "* The prediction error is calculated as:\n",
    "\n",
    "$$\\text{error} = \\frac{4 + 2 + 5}{30+40+80} = \\frac{11}{150} \\approx 7.3 \\%$$\n",
    "\n",
    "\n",
    "* The classification accuracy doesn't really gives an insight on which class is being misclassified the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "A confusion matrix summarizes the classification accuracy across several classes. It shows the ways in which the classification model is confused when it makes predictions, allowing visualization of the performance of our algorithm. \n",
    "\n",
    "Generally, each row represents the instances of a actual class while each column represents the instances in an predicted class.\n",
    "\n",
    "If the classifier is trained to distinguish between salmon, sea bass and cod. We can summarize the prediction result in the confusion matrix as follows:\n",
    "\n",
    "|actual/predict|    salmon    |    sea bass  |      cod     |\n",
    "|--------------|--------------|--------------|--------------|\n",
    "|    salmon    |      26      |       2      |       2      |\n",
    "|    sea bass  |       2      |       38     |       0      |\n",
    "|      cod     |       2      |       3      |       75     |\n",
    "\n",
    "\n",
    "In this confusion matrix, of the 30 salmons (row 1), the classifier predicted that 26 are labeled salmon correctly, 2 are wrongly labeled as sea bass, and another 2 are wrongly labeled as cod. \n",
    "\n",
    "All correct predictions are located in the diagonal of the table. So it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Recall & Fall-Out\n",
    "\n",
    "We are often looking to discriminate between observations with a specific binary outcome, for example, event or no event. In our example, the fish company would like to produce salmon can but the harvest contains all three species. In this way,\n",
    "we can assign the event (salmon) as \"positive\" and no-event (not salmon) as \"negative\".\n",
    "\n",
    "The confusion matrix for this two-class classification problem is:\n",
    "\n",
    "|actual/predict|    salmon    |  non-salmon  |\n",
    "|--------------|--------------|--------------|\n",
    "|    salmon    |      26      |       4      |\n",
    "|  non-salmon  |       7      |      113     |\n",
    "\n",
    "* **True positive (TP):** correctly predicting positive events\n",
    "* **False positive (FP):** incorrectly calling positive to a negative event\n",
    "* **True negative (TN):** correctly predicting negative events\n",
    "* **False negative (FN):** incorrectly labeling negative to a positive event\n",
    "\n",
    "*In this salmon/non-salmon classification problem, what are the TP, FP, TN, FN values?*\n",
    "\n",
    "|actual/predict|   Positive   |   Negative   |\n",
    "|--------------|--------------|--------------|\n",
    "|   Positive   |      TP      |      FN      |\n",
    "|   Negative   |      FP      |      TN      |\n",
    "\n",
    "* **Precision**, also called Positive Predictive Value (PPV), is the performance of detection\n",
    "\n",
    "$$\\text{Precision} = \\text{PPV} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "* **Recall**, also called True Positive Rate (TPR) or Sensitivity, is the probability of detection\n",
    "\n",
    "$$\\text{Recall} = \\text{TPR} = \\text{Sensitivity} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "* **Fall-out**, also called False Positive Rate (FPR), is the probability of false alarm\n",
    "\n",
    "$$\\text{Fall-out} = \\text{FPR} = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "* **Specificity**, also called True Negative Rate (TNR), is the probability of negative events detection\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "<!--* **F1-score**, also called F-score or F-measure, is a measure of a model's accuracy. It considers both the precision and the recall-->\n",
    "\n",
    "<!--$$\\text{F1-score} = 2\\times\\frac{\\text{Precision}\\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$-->\n",
    "\n",
    "* Learn about many other measures on the [Wikipedia page](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) and [Scikit-Learn Metrics Module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curves\n",
    "\n",
    "**Receiver Operating Characteristic (ROC) curve** is the plot between the true positive rate (TPR) and the false positive rate (FPR), where the TPR is defined as the y-axis and FPR is defined as the x-axis.\n",
    "\n",
    "* ROC curves were first developed for RADAR systems, hence the name.\n",
    "\n",
    "* Given a binary classifier and its threshold, the (x,y) coordinates of ROC space can be calculated from all the prediction result. You trace out a ROC curve by varying the threshold to get all of the points on the ROC.\n",
    "\n",
    "* The diagonal between (0,0) and (1,1) separates the ROC space into two areas, which are left up area and right bottom area. The points above the diagonal represent good classification (better than random guess) which below the diagonal represent bad classification (worse than random guess).\n",
    "\n",
    "* *What is the perfect prediction point in a ROC curve?*\n",
    "\n",
    "\n",
    "## Area Under the Curve (AUC)\n",
    "\n",
    "**Area Under Curve (AUC)** is a common measure of how good a test is. It is simply the area under the ROC curve. Random guessing can achieve the diagonal line, so the minimum AUC is 1/2. The maximum AUC is 1, which is achieved by a test that is always right; the ROC curve is along the left and top axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "1. Suppose you have a target detection task that you would like to evaluate using ROC curve analysis. You emplaced 10 targets and collected aerial hyperspectral imagery over 10 $km^2$. Then, suppose you ran a set of alarm generation and target detection algorithms over the collected data. Your algorithms produced the following list of alarm confidence values. You have already matched each of these alarms to a location on the ground and compared them with you ground truth. True targets, based on your ground truth, are marked with a \"T\" in the second column. Draw the associated ROC cure for these results.\n",
    "\n",
    "Alarm confidence values |  0.91  |  0.90  |  0.80  |  0.79  |  0.77  |  0.75  |  0.50  |  0.40  |  0.39  |  0.38  |  0.37  |  0.25  |  0.10  |  0.09  |  0.01  |\n",
    "------------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n",
    "    Ground truth        |   T    |   T    |        |   T    |        |        |        |   T    |        |        |        |        |        |   T    |        |\n",
    "\n",
    "\n",
    "2. Suppose you were segmenting a data set into three classes (e.g., vegetation, man-made materials, sand) and wanted to evaluate your results. Would using a ROC curve be an appropriate method for evaluation? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
