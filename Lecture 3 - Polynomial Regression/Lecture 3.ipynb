{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3 - Linear Regression with Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last class, we introduced different types of learning, in particular, supervised learning and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also described a typical workflow of a supervised learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "**Supervised Learning:** Learning a mapping from input data to desired output values given labeled training data.\n",
    "\n",
    "The typical workflow of a Supervised Learning algorithm is as follows:\n",
    "\n",
    "**Training Stage**\n",
    "1. Collect labeled training data - often the most time-consuming and expensive task.\n",
    "2. Extract features - extract *useful* features from the input (or observational) data such that they have discriminatory information in successfully mapping the desired output. \n",
    "3. Select a model - relationship between input data and desired output.\n",
    "4. Fit the model - change model parameters (*Learning Algorithm*) in order to meet some *Objective Function*.\n",
    "\n",
    "**Testing Stage**\n",
    "1. Given unlabeled test data\n",
    "2. Extract (the same) features\n",
    "3. Run the unabeled data through the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block-diagram for Supervised Learning\n",
    "\n",
    "<div><img src=\"figures/SupervisedLearning.png\", width=\"800\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned some of the challenges of supervised learning:\n",
    "\n",
    "* How do you know if you have *representative* training data?\n",
    "* How do you know if you extracted *good* features?\n",
    "* How do you know if you selected the *right* model?\n",
    "* How do you know if you trained the model *well*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As so, a lot of Machine Learning scientists' work is to select:\n",
    "\n",
    "1. which **features** to collect or generate\n",
    "2. which **mapper** function to use\n",
    "3. which **objective function** (or **cost function**) to use\n",
    "4. which **learning algorithm** to use\n",
    "\n",
    "After these choices are made, the machine will make use of **feedback** loop and update itself as it is given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it is *extremelly* important that we have enough data (in order to have accurate representations) but also *good* data. And this is one of the most time consuming and, arguably, the most important step in Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Polynomial Regression** is a type of linear regression that uses a special set of *features* - polynomial features.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 1 - Input Space</b> \n",
    "\n",
    "Suppose we are given a training set comprising of $N$ observations of $\\mathbf{x}$, $\\mathbf{x} = \\left[x_1, x_2, \\ldots, x_N \\right]^T$, and its corresponding desired outputs $\\mathbf{t} = \\left[t_1, t_2, \\ldots, t_N\\right]^T$, where sample $x_i$ has the desired label $t_i$.\n",
    "\n",
    "So, we want to learn the *true* function mapping $f$ such that $\\mathbf{t}  = f(\\mathbf{x}, \\mathbf{w})$, where $\\mathbf{w}$ are *unknown* parameters of the model.\n",
    "</div>\n",
    "\n",
    "* We generally organize data into *vectors* and *matrices*. Not only is it a common way to organize the data, but it allows us to easily apply linear algebraic operations during analysis. It also makes it much simpler when it comes to code implementation!\n",
    "    * In engineering textbooks and in this course's notation, **vectors** are defined as *column vectors*. This is why we write $\\mathbf{x} = \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N\\end{array} \\right] = \\left[x_1, x_2, \\ldots, x_N \\right]^T$.\n",
    "\n",
    "* Note that both the training data and desired outputs can be noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 2 - Feature Space</b> \n",
    "\n",
    "For the polynomial regression problem, let's consider *polynomial features* for each data point $x_i$. Let's say we can find these features using a **basis function**, $\\phi(\\mathbf{x})$. In the *polynomial regression* example, let's consider $\\phi(x_i) = \\left[ x_{i}^{0}, x_{i}^{1}, x_{i}^{2}, \\dots, x_{i}^{M}\\right]^T$, where $x_i^M$ is the $M^{th}$-power of $x_i$.\n",
    "</div>\n",
    "\n",
    "* These are particularly called **polynomial features** but other features can be extracted.\n",
    "\n",
    "* For all data observations $\\{x_i\\}_{i=1}^N$ and using the feature space defined as $\\phi(x_i) = \\left[x_{i}^{0}, x_{i}^{1}, x_{i}^{2}, \\dots, x_{i}^{M}\\right]^T$, we can write the input data in a *matrix* form as:\n",
    "\n",
    "$$\\mathbf{X} =\\left[\\begin{array}{c} \\phi(x_1)^T \\\\ \\phi(x_2)^T \\\\ \\vdots \\\\ \\phi(x_N)^T \\end{array}\\right]  = \\left[\\begin{array}{ccccc}\n",
    "1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{M}\\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{M}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{M}\n",
    "\\end{array}\\right] \\in \\mathbb{R}^{N\\times (M+1)}$$\n",
    "\n",
    "where each row is a feature representation of a data point $x_i$.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Feature Space</b> \n",
    "\n",
    "The set of features drawn by the transformation \n",
    "\n",
    "\\begin{align}\n",
    "\\phi: \\mathbb{R}^D & \\rightarrow \\mathbb{R}^M \\\\\n",
    "\\mathbf{x} & \\rightarrow \\left[x_{i}^{0}, x_{i}^{1}, x_{i}^{2}, \\dots, x_{i}^{M}\\right]^T\n",
    "\\end{align}\n",
    "is often called the **feature space**.\n",
    "When we write a linear regression with respect to a set of basis functions, the regression model is linear in the *feature space*.\n",
    "\n",
    "$M$ is dimensionality of the feature space and is often called the *model order*.\n",
    "</div>\n",
    "\n",
    "* Now, we want to find the mapping from the feature input data $\\mathbf{X}$ to the desired output values $\\mathbf{t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the data actually comes from some **unknown hidden function**, that takes in the data points $\\mathbf{x}$ with some parameters $\\mathbf{w}$ and produces the desired values $\\mathbf{t}$, i.e. $\\mathbf{t} = f(\\mathbf{x},\\mathbf{w})$.\n",
    "* We do not know anything about the function $f$. If we knew the hidden function, we would not need to learn the *mapping* - we would already know it. However, since we do not know the true underlying function, we need to do our best to estimate from the examples of input-output pairs that we have.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 3 - Model Selection or Mapping</b> \n",
    "\n",
    "Let's assume that the desired output values are a *linear combination* of the feature input space, i.e., a **linear regression model** of **polynomial features**\n",
    "\n",
    "$$t \\sim f(x,\\mathbf{w}) = w_0x^0 + w_1x^1 + w_2x^2+\\cdots+w_Mx^M = \\sum_{j=0}^M w_jx^j = \\mathbf{X}\\mathbf{w}$$\n",
    "</div>\n",
    "\n",
    "* This means that for every paired training data point $\\{x_i, t_i\\}_{i=1}^N$, we can model the output value as \n",
    "\n",
    "$$t_i \\sim f(x_i,\\mathbf{w}) = w_0x_i^0 + w_1x_i^1 + w_2x_i^2+\\cdots+w_Mx_i^M $$\n",
    "\n",
    "* Although the polynomial function $f(x,\\mathbf{w})$ is a nonlinear function of $x$, it is a linear function of the coefficients $\\mathbf{w}$. Functions, such as the polynomial, which are linear in the unknown parameters have important properties and are called *linear models*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the coefficients $\\mathbf{w}$ will be determined by *fitting* the polynomial to the training data. \n",
    "\n",
    "This can be done by minimizing an **error function** (also defined as **cost function**, **objective function**, or **loss function**) that measures the *misfit* between the function $f(x,\\mathbf{w})$, for any given value of $\\mathbf{w}$, and the training set data points $\\{x_i,t_i\\}_{i=1}^N$.\n",
    "\n",
    "* What is the model's *objective* or goal?\n",
    "\n",
    "<div><img src=\"figures/LeastSquares.png\", width=\"300\"><!div>\n",
    "\n",
    "One simple choice for fitting the model is to consider the error function given by the sum of the squares of the errors between the predictions $f(x_i,\\mathbf{w})$ for each data point $x_i$ and the corresponding target values $t_i$, so that we minimize\n",
    "\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N \\left(f(x_n,\\mathbf{w}) - t_n\\right)^2 = \\frac{1}{2}\\left\\Vert f(\\mathbf{x},\\mathbf{w}) - \\mathbf{t} \\right\\Vert^2_2$$\n",
    "\n",
    "* This error function is minimizing the (Euclidean) *distance* of every point to the curve.\n",
    "\n",
    "* **What other/s objective function can we use?**\n",
    "\n",
    "* We can write the error function compactly in matrix/vector form:\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2}\\left\\Vert f(\\mathbf{x},\\mathbf{w}) - \\mathbf{t} \\right\\Vert^2_2 \\\\\n",
    "&= \\frac{1}{2} \\left\\Vert \\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right\\Vert^2_2\\\\\n",
    "&= \\frac{1}{2} \\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right)^T \\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right)\\\\\n",
    "\\text{where: } & \\mathbf{X} = \\left[\\begin{array}{ccccc}\n",
    "1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{M}\\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{M}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{M}\n",
    "\\end{array}\\right], \\mathbf{w} =  \\left[\\begin{array}{c}\n",
    "w_{0}\\\\\n",
    "w_{1}\\\\\n",
    "\\vdots\\\\\n",
    "w_{M}\n",
    "\\end{array}\\right], \\text{and }  \\mathbf{t} = \\left[\\begin{array}{c}\n",
    "t_{1}\\\\\n",
    "t_{2}\\\\\n",
    "\\vdots\\\\\n",
    "t_{N}\n",
    "\\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 4 - Fitting the Model or Learning Algorithm</b> \n",
    "\n",
    "Also referred as training the model.\n",
    "\n",
    "We *fit* the polynomial function model such that the *objective function* $J(\\mathbf{w})$ is minimized, i.e. we *optimize* the following error function\n",
    "\n",
    "\\begin{align}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right)^T \\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right) \\\\\n",
    "&= \\frac{1}{2} \\left\\Vert \\mathbf{X}\\mathbf{w} - \\mathbf{t} \\right\\Vert_2^2\n",
    "\\end{align}\n",
    "\n",
    "* This function is called the **least squares error** objective function.\n",
    "\n",
    "The learning algorithm function is then:\n",
    "$$\\arg_{\\mathbf{w}}\\min J(\\mathbf{w})$$ \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We *optimize* the error function $J(\\mathbf{w})$ in order to find the *optimal* set of parameter $\\mathbf{w}^*$ that minimize the objective function.\n",
    "\n",
    "* To do that, we **take the derivative of $J(\\mathbf{w})$ with respect to the parameters $\\mathbf{w}$**.\n",
    "\n",
    "* How do you take the derivative of a *scalar*, such as $J(\\mathbf{w})$, with respect to a vector, such as $\\mathbf{w}$?\n",
    "\n",
    "We can use matrix calculus directly to compute that derivative, let's instead take a look at other ways of finding *a* solution using Linear Algebra tools:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Linear Vector-valued Functions and Least Squares Solution\n",
    "\n",
    "Consider the linear vector function defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{f}: \\mathbb{R}^{M+1} &\\longrightarrow \\mathbb{R}^N \\\\\n",
    "\\mathbf{w} &\\longmapsto \\mathbf{X}\\mathbf{w}\n",
    "\\end{align}\n",
    "\n",
    "This is our **mapper** function when our model is a **Linear Regression**.\n",
    "\n",
    "* We want to find the solution for:\n",
    "\n",
    "$$\\mathbf{X}\\mathbf{w} = \\mathbf{t}$$\n",
    "\n",
    "where $\\mathbf{X}$ is a *tall* matrix of polynomial features, $\\mathbf{w}$ is the vector of unknown parameters, and $\\mathbf{t}$ is the vector of labels.\n",
    "\n",
    "* We **cannot** simply take the left-inverse to find $\\mathbf{w}$ because the feature matrix $\\mathbf{X}$ is not square.\n",
    "\n",
    "* As we typically have a lot more samples $N$ than unknown parameters $M+1$ ($N>>M+1$), this is an over-constrained problem and so we do not have a solution. \n",
    "\n",
    "* We can find a solution using the Least Squares approximation, that is, find the set of coefficients $\\mathbf{w}$ that minimize the squared error approximation:\n",
    "\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2} \\left\\Vert \\mathbf{X}\\mathbf{w} - \\mathbf{t} \\right\\Vert_2^2$$\n",
    "\n",
    "* By taking the derivative of $J(\\mathbf{w})$ and solving for $\\mathbf{w}$, the **least squares solution** is then given by:\n",
    "\n",
    "$$\\mathbf{w} = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}$$\n",
    "\n",
    "Where $\\mathbf{X}^{\\dagger} = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}$ is the **pseudo-inverse** (or **Moore–Penrose inverse**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Polynomial Regression\n",
    "\n",
    "After we have decided the on the cost function and learning algorithm, what are the programmable parameters we will need to implement a Polynomial Regression model?\n",
    "\n",
    "* Training data, and\n",
    "* Model order\n",
    "\n",
    "**How would you implement linear regression using polynomial features?**\n",
    "\n",
    "Let's take a look at the pseudocode below:\n",
    "\n",
    "<div><img src=\"figures/LinearRegressionPseudocode.png\", width=\"600\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's simulated some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Suppose the input training data is sampled from a noisy sine curve.\n",
    "\n",
    "Suppose our data comes from a noisy sinusoidal: $t = \\sin(2\\pi x) + \\epsilon$ where $\\epsilon$ is a (univariate) Gaussian zero-mean random noise. \n",
    "\n",
    "* The input samples are $x$\n",
    "* The desired values are $t + \\epsilon$, but we know that $t = \\sin(2\\pi x)$\n",
    "* Our **goal** is to find a model that fits the set of data samples $\\{x_i,t_i\\}_{i=1}^N$\n",
    "* Also, we want our model to be able to correctly **predict** the desired value of a new data sample $x_{test}$\n",
    "\n",
    "Let's generate data from the *true* underlying function, $t=\\sin(2\\pi x)$, which, in practice, we would not know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                # numpy contains a lot of numerical and linear algebra function\n",
    "import numpy.random as npr        # module of numpy that contains functions to generate random phenomena\n",
    "\n",
    "import matplotlib.pyplot as plt   # plotting library\n",
    "%matplotlib inline                \n",
    "# built-in magic to make sure the plot are statically embedded in the output cells\n",
    "plt.style.use('seaborn-colorblind')          # style sheet for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisySinusoidalData(N, a, b, gVar):\n",
    "    x = np.linspace(a,b,N)\n",
    "    noise = npr.normal(0,gVar,N)\n",
    "    t = np.sin(2*np.pi*x) + noise\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will generate synthetic data (data points and labels). Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40 \n",
    "Ntest = 10 \n",
    "a, b = [0,1] \n",
    "gVar_train = 0.5 \n",
    "gVar_test = 1 \n",
    "x1, t1 = NoisySinusoidalData(N, a, b, gVar_train)\n",
    "x2, t2 = NoisySinusoidalData(N, a, b, 0)\n",
    "x3, t3 = NoisySinusoidalData(Ntest, a, b, gVar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x1, t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x2, t2, 'g', label = 'True Sinusoidal')\n",
    "plt.plot(x3, t3, 'ro', label = 'Test Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x', fontsize=15)\n",
    "plt.ylabel('Desired Values, t', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We want to fit the (blue) training data samples. And we want to do it such that we are *still able* to fit the (red) test data points (without labels).\n",
    "\n",
    "Now let's fit the data using a **polynomial regression** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 3\n",
    "\n",
    "w, y, error = PolynomialRegression(x1,t1,M) \n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x1,y,'k', label = 'Estimated Polynomial')\n",
    "plt.plot(x2,t2,'g', label = 'True Function')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x', fontsize=15)\n",
    "plt.ylabel('Desired Values, t', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do the weights look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We trained the model!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to used this trained model (with the parameters the machine learned, call then $\\mathbf{w}^*$) and applied it to a test set without labels.\n",
    "\n",
    "In the test data:\n",
    "* Apply the same feature extraction as in training: $\\mathbf{X}_{test}$, where $\\mathbf{X}_{test}$ is a $K\\times (M+1)$ feature matrix, and $K$ the number of test points\n",
    "* Predict the output using $\\mathbf{w}^*$: $$\\mathbf{y}_{test} = \\mathbf{X}_{test}\\mathbf{w}^*$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = PolynomialRegression_test(x3, M, w)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x1,y,'k', label = 'Estimated Polynomial')\n",
    "plt.plot(x2,t2,'g', label = 'True Function')\n",
    "plt.plot(x3,t3,'ro', label = 'Test Data')\n",
    "plt.plot(x3,y_test,'-r*', label = 'Test Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x', fontsize=15)\n",
    "plt.ylabel('Desired Values, t', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What happens when the test points fall outside the range of values of what the model has *learned*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4, t4 = NoisySinusoidalData(10, 0.5, 1.5, 0.1)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x1, t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x2, t2, 'g', label = 'True Sinusoidal')\n",
    "plt.plot(x4, t4, 'ro', label = 'Test Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x', fontsize=15)\n",
    "plt.ylabel('Desired Values, t', fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = PolynomialRegression_test(x4, M, w)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x1,y,'r', label = 'Estimated Polynomial')\n",
    "plt.plot(x2,t2,'g', label = 'True Function')\n",
    "plt.plot(x3,t3,'ro', label = 'Test Data')\n",
    "plt.plot(x4,y_test,'k', label = 'Test Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x', fontsize=15)\n",
    "plt.ylabel('Desired Values, t', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "* As the model order increases, the model fit deteriorates as it will *memorize* the training data\n",
    "* The model will only be able to represent the regions of the input space that are present in the training data\n",
    "* This leads to poor *generalization* ability, that is, the model cannot make predictions outside what it has seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption of Least Squares Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least squares objective function, $J(\\mathbf{w})$ assumes Gaussian noise! That is, it finds the solution for $\\mathbf{w}$ such that the residual errors, $\\mathbf{e}=\\mathbf{t}-\\mathbf{y}$, were drawn from a Gaussian distribution.\n",
    "\n",
    "* This may not be true for (most) real world data! \n",
    "\n",
    "* But it is an assumption we are willing to take as it simplifies implementations and allows to fast and easy derivations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider our optimization function:\n",
    "\n",
    "\\begin{align}\n",
    "& \\arg_{\\mathbf{w}}\\min J(\\mathbf{w})\\\\\n",
    "&= \\arg_{\\mathbf{w}} \\max -J(\\mathbf{w}) \\\\\n",
    "&\\propto \\arg_{\\mathbf{w}} \\max \\exp\\left(-J(\\mathbf{w})\\right),\\text{ because }\\exp\\text{ is a monotonically increasing function}\\\\\n",
    "&=\\arg_{\\mathbf{w}} \\max \\exp\\left(-\\frac{1}{2}(\\mathbf{t}-\\mathbf{y})^2\\right)\\\\\n",
    "&=\\arg_{\\mathbf{w}} \\max \\exp\\left(-\\frac{1}{2}\\mathbf{e}^2\\right)\\\\\n",
    "&\\propto \\mathcal{N}(0,1)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "* Which model order would you choose?\n",
    "* How do you *validate* your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
