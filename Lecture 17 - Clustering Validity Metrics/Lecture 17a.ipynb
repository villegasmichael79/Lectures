{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 17a - Cluster Validity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Applications\n",
    "\n",
    "K-Means is a very popular algorithm and is commonly used in a variety of applications, such as: market segmentation, document clustering, image segmentation, image compression, etc.\n",
    "\n",
    "The *goal* usually when we undergo a cluster analysis is either:\n",
    "\n",
    "1. Get a meaningful intuition of the structure of the data we are dealing with.\n",
    "\n",
    "2. Cluster-then-predict where different models will be built for different subgroups. An example of that is clustering patients into different subgroups (based on some feature map) and build a model for each subgroup to predict the probability of the risk of having a heart attack.\n",
    "\n",
    "Let's take a look at two case applications for K-Means: Image Compression and Data Segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: K-Means as Data Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from scipy.spatial.distance import cdist\n",
    "from numpy.random import permutation\n",
    "\n",
    "def KMeans(X, K, MaxIter = 10000, Thresh = 1e-5, Visualization = True):\n",
    "    '''This function implements K-Means algorithm. \n",
    "    Where the data X will be partitioned into K clusters.\n",
    "    Data matrix X is of size NxD, where N is the number of points and D the dimension of the data.'''\n",
    "\n",
    "    #Initialize Cluster Centers by drawing Randomly from Data (can use other\n",
    "    # methods for initialization...)\n",
    "    N       = X.shape[0] # number of data points\n",
    "    rp      = np.random.permutation(N); # shuffle the data (random permutation)\n",
    "    centers = X[rp[0:K],:]; # select first K data points as cluster centroids\n",
    "    \n",
    "    if Visualization:\n",
    "        fig = plt.figure()\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.scatter(X[:,0],X[:,1],c='b')\n",
    "        plt.plot(centers[:,0],centers[:,1],'xr',markersize=20)\n",
    "        plt.title('Randomly select K='+str(K)+' data points as Centroids')\n",
    "        plt.show()\n",
    "    \n",
    "    diff    = np.inf;\n",
    "    iter    = 0;\n",
    "    while((diff > Thresh) & (iter < MaxIter)):\n",
    "        D   = spatial.distance.cdist(X, centers) # compute the distance of every point to every cluster centroid\n",
    "        L   = np.argmin(D, axis=1) # Assign data to closest cluster representative (using Euclidean distance)\n",
    "\n",
    "        # Visualization\n",
    "        if Visualization:\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.scatter(X[:,0],X[:,1],c=L)\n",
    "            plt.plot(centers[:,0],centers[:,1],'xr',markersize=20)\n",
    "            plt.title('Membership Assignment')\n",
    "            \n",
    "        #Update cluster centers\n",
    "        centersPrev = centers.copy()\n",
    "        for i in range(K):\n",
    "            centers[i,:] = np.mean(X[L == i,:], axis=0) # New cluster centroids will be the average of all points assigned to it\n",
    "\n",
    "        # Visualization\n",
    "        if Visualization:\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.scatter(X[:,0],X[:,1],c=L)            \n",
    "            plt.plot(centers[:,0],centers[:,1],'xr',markersize=20)\n",
    "            plt.title('Update Cluster Centroids')\n",
    "            plt.pause(1)\n",
    "        \n",
    "        #Update diff & iteration count for stopping criteria\n",
    "        diff = np.linalg.norm(centersPrev - centers)\n",
    "        iter = iter+1\n",
    "    return centers, L\n",
    "\n",
    "# Plotting function for synthetically generated data\n",
    "def Plotting(X, true_labels, L, name):\n",
    "    if len(true_labels)>0:\n",
    "        fig = plt.figure(figsize=(12,4))\n",
    "        fig.add_subplot(1,2,1)\n",
    "        plt.scatter(X[:,0],X[:,1],c=true_labels)\n",
    "        plt.title(name)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(12,4))\n",
    "        fig.add_subplot(1,2,1)\n",
    "        plt.scatter(X[:,0],X[:,1],c='b')\n",
    "        plt.title(name)\n",
    "    fig.add_subplot(1,2,2)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=L)\n",
    "    plt.title(\"K-Means Clustering Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Create synthetically generate data\n",
    "n_samples = 1500\n",
    "X1, T1 = datasets.make_blobs(n_samples=n_samples,centers=3,cluster_std=1)               # Blobs data\n",
    "X2, T2 = datasets.make_blobs(n_samples=n_samples,cluster_std=[1.0, 2.5, 0.5],centers=3) # Different Variance Blobs data\n",
    "X3, T3 = datasets.make_moons(n_samples=n_samples, noise=.05)                            # Moons data\n",
    "X4, T4 = datasets.make_circles(n_samples, noise=.05, factor=0.5)                        # Circles data\n",
    "X5     = np.dot(X1, [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]])             # Anisotropicly distributed data\n",
    "X6     = np.vstack((X1[T1 == 0][:500], X1[T1 == 1][:100], X1[T1 == 2][:10]))            # Unevenly sized Blobs data\n",
    "T6     = np.hstack((np.zeros(500),np.ones(100),2*np.ones(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NoClusters = 3\n",
    "C1, L1 = KMeans(X1, NoClusters, Visualization = True)\n",
    "Plotting(X1, T1, L1, 'Blobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NoClusters = 3\n",
    "C2, L2 = KMeans(X2, NoClusters, Visualization = True)\n",
    "Plotting(X2, T2, L2, 'Different Variance Blobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoClusters = 2\n",
    "C3, L3 = KMeans(X3, NoClusters, Visualization = False)\n",
    "Plotting(X3, T3, L3, 'Moons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoClusters = 2\n",
    "C4, L4 = KMeans(X4, NoClusters, Visualization = False)\n",
    "Plotting(X4, T4, L4, 'Circles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoClusters = 3\n",
    "C5, L5 = KMeans(X5, NoClusters, Visualization = False)\n",
    "Plotting(X5, T1, L5, 'Anisotropicly distributed data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NoClusters = 3\n",
    "C6, L6 = KMeans(X6, NoClusters, Visualization = False)\n",
    "Plotting(X6, T6, L6, 'Unevenly sized Blobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: K-Means as Image Compression\n",
    "\n",
    "*Example and code from book [\"Data Science Handbook\" by Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)*\n",
    "\n",
    "One interesting application of clustering is in color compression within images. For example, imagine you have an image with millions of colors. In most images, a large number of the colors will be unused, and many of the pixels in the image will have similar or even identical colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "image = load_sample_image('china.jpg')\n",
    "ax = plt.axes(xticks=[], yticks=[])\n",
    "ax.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image itself is stored in a three-dimensional array of size (height, width, RGB), containing red/blue/green contributions as integers from 0 to 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can view this set of pixels is as a cloud of points in a three-dimensional color space. We will reshape the data to $[$n_samples $\\times$ n_features$]$, and rescale the colors so that they lie between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = image / 255.0 # use 0...1 scale\n",
    "data = data.reshape(427 * 640, 3)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize these pixels in this color space, using a subset of 10,000 pixels for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pixels(data, title, colors=None, N=10000):\n",
    "    if colors is None:\n",
    "        colors = data\n",
    "    \n",
    "    # choose a random subset\n",
    "    rng = np.random.RandomState(0)\n",
    "    i = rng.permutation(data.shape[0])[:N]\n",
    "    colors = colors[i]\n",
    "    R, G, B = data[i].T\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    ax[0].scatter(R, G, color=colors, marker='.')\n",
    "    ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "    ax[1].scatter(R, B, color=colors, marker='.')\n",
    "    ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "    fig.suptitle(title, size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pixels(data, title='Input color space: 16 million possible colors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reduce these 16 million colors to just 16 colors, using K-Means clustering across the pixel space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as KMeans_sklearn\n",
    "\n",
    "kmeans = KMeans_sklearn(16)\n",
    "kmeans.fit(data)\n",
    "new_colors = kmeans.cluster_centers_[kmeans.predict(data)]\n",
    "\n",
    "plot_pixels(data, colors=new_colors, title=\"Reduced color space: 16 colors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a re-coloring of the original pixels, where each pixel is assigned the color of its closest cluster center. Plotting these new colors in the image space rather than the pixel space shows us the effect of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_recolored = new_colors.reshape(image.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6),subplot_kw=dict(xticks=[], yticks=[]))\n",
    "fig.subplots_adjust(wspace=0.05)\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title('Original Image', size=16)\n",
    "ax[1].imshow(image_recolored)\n",
    "ax[1].set_title('16-color Image', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some detail is certainly lost in the rightmost panel, but the overall image is still easily recognizable. This image on the right achieves a compression factor of around 1 million! While this is an interesting application of k-means, there are certainly better ways to compress information in images. But the example shows the power of thinking outside of the box with unsupervised methods like k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Validity Metrics\n",
    "\n",
    "How would you evaluate clustering results? - **Cluster Validity Indices**\n",
    "    \n",
    "* Cluster validity indices are used for a number of different goals. For example, cluster validity metrics can be used to compare clustering results, try to determine the *correct* number of clusters, try to select the *correct* parameter settings, try to evaluate the approppriateness of the clustering result based on the data only (and not using another result or \"ground truth\" data).\n",
    "    \n",
    "In general, there are three types of **index criteria** to perform cluster validity:\n",
    "\n",
    "1. **Internal criteria.** We evaluate the results of a clustering algorithm in terms of quantities that involve the vectors of the data set themselves. \n",
    "2. **External criteria.** We evaluate the results of a clustering algorithm based on a pre-specified structure, which is imposed on a data set and reflects our intuition about the clustering structure of the data set.\n",
    "3. **Relative criteria.** We evaluate the results of a clustering structure by comparing it to other clustering schemes, resulting by the same algorithm but with different parameter values. In practice, relative criteria are a combination on internal and external criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal Criteria \n",
    "\n",
    "As the goal of clustering is to make objects within the same cluster similar and objects in different clusters distinct, internal cluster validity measures are defined by combining compactness and separability.\n",
    "\n",
    "The optimal clustering scheme under the internal criteria index includes:\n",
    "\n",
    "* Compactness (or intra-distance or within-cluster scatter): The members of each cluster should be as close to each other as possible. A common measure of compactness is the variance, which should be minimized.\n",
    "* Separation (or inter-distance or between-cluster scatter): This indicates how distinct two clusters are. It computes the distance between two different clusters. There are three common approaches measuring the distance between two different clusters:\n",
    "    * Single linkage: It measures the distance between the closest members of the clusters. \n",
    "    * Complete linkage: It measures the distance between the most distant members. \n",
    "    * Comparison of centroids: It measures the distance between the centers of the clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Silhouette Index\n",
    "\n",
    "The Silhoute Index is an internal cluster validity index that is used to judge the quality of any clustering solution. \n",
    "\n",
    "Given a set of data points $X=\\{x_1,\\dots,x_N\\}$ and a partition of $X$ (i.e. clustering result). Let's define the following:\n",
    "* $a_i$ is the average distance of the point $x_i$ to all the other points of the cluster in which $x_i$ is assigned to\n",
    "* $b_i$ is the average distance of the point $x_i$ to all the other points of in the other clusters. \n",
    "\n",
    "For every data point $x_i \\in X$, the Silhouette Index is defined as:\n",
    "\n",
    "$$s = \\frac{1}{N} \\sum_{i=1}^N \\frac{b_i-a_i}{\\max(a_i,b_i)}$$\n",
    "\n",
    "* Silhouette index is the average silhouette of all data points and it reflects the compactness and separation of clusters.\n",
    "\n",
    "* The value of silhouette index varies from -1 and 1 and higher indicates better clustering results.\n",
    "\n",
    "There are many other internal cluster validity indices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Criteria\n",
    "\n",
    "External cluster validity indices are used to measure how well a clustering result matches a set of *give* labels. \n",
    "External cluster validity indices can be used to:\n",
    "* compare the clustering results with the *ground truth* (true labels),\n",
    "* compare clustering results between different clustering algorithms to measure how different they are and how stable a particular clustering is on a data set across parameter settings and/or algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Rand Index\n",
    "\n",
    "The Rand Index is an external cluster validity index that is used to compare clustering results obtained from different parameter settings or algorithms. \n",
    "\n",
    "Given a set of data points $X$ and two partitions (i.e. clustering results) of $X$ to compare. One partition $C=\\{C_1, \\dots,C_k\\}$, that partitions $X$ into $k$ clusters, and another partition $D=\\{D_1,\\dots,D_s\\}$, that partitions $X$ into $s$ clusters. Let's define the following:\n",
    "\n",
    "* $a$ is the number of pairs of elements in $X$ that are in the same subset in $C$ and in the same subset in $D$.\n",
    "* $b$ is the number of pairs of elements in $X$ that are in different subset in $C$ and in different subset in $D$.\n",
    "* $c$ is the number of pairs of elements in $X$ that are in the same subset in $C$ and in different subset in $D$.\n",
    "* $d$ is the number of pairs of elements in $X$ that are in different subset in $C$ and in the same subset in $D$.\n",
    "\n",
    "The Rand Index is defined as:\n",
    "\n",
    "$$r = \\frac{a+b}{a+b+c+d}$$\n",
    "\n",
    "* Intuitively, $a+b$ can be considered as the number of *agreements* between $C$ and $D$, and $c+d$ as the number of *disagreements* between $C$ and $D$.\n",
    "\n",
    "* The value of rand index varies from 0 and 1 and higher indicates higher consistency between partitions $C$ and $D$.\n",
    "\n",
    "There are many other external cluster validity indices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
