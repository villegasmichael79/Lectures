{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 - (continued) Polynomial Curve Fitting & Experimental Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last class, we have determined all steps for training a Polynomial Regression model.\n",
    "\n",
    "We have also implemented this model in Python and fitted a polynomial curve onto a noisy sine curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Curve Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose our data comes from a noisy sinusoidal: $t = \\sin(2\\pi x) + \\epsilon$ where $\\epsilon$ is a (univariate) Gaussian zero-mean random noise. \n",
    "\n",
    "* The input samples are $x$\n",
    "* The desired values are $t + \\epsilon$, but we know that $t = \\sin(2\\pi x)$\n",
    "* Our **goal** is to find a model that fits the set of data samples $\\{x_i,t_i\\}_{i=1}^N$\n",
    "* We also want our model to be able to correctly **predict** the desired value of a new data sample $x_{test}$\n",
    "\n",
    "Let's generate data from the *true* underlying function, $t=\\sin(2\\pi x)$, which, in practice, we would not know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisySinusoidalData(N, a, b, gVar):\n",
    "    x = np.linspace(a,b,N)\n",
    "    noise = npr.normal(0,gVar,N)\n",
    "    t = np.sin(2*np.pi*x) + noise\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40\n",
    "Ntest = 10 \n",
    "a, b = [0,1] \n",
    "gVar_train = 0.5 \n",
    "gVar_test = 1 \n",
    "x1, t1 = NoisySinusoidalData(N, a, b, gVar_train)    # training data and labels\n",
    "x2, t2 = NoisySinusoidalData(N, a, b, 0)             #true sine function\n",
    "x3, t3 = NoisySinusoidalData(Ntest, a, b, gVar_test) # test data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x1, t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x2, t2, 'g', label = 'True Sinusoidal')\n",
    "plt.plot(x3, t3, 'ro', label = 'Test Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x', fontsize=15)\n",
    "plt.ylabel('Desired Values, t', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap on Training Stage steps for Polynomial Regression\n",
    "\n",
    "**Training Stage**\n",
    "\n",
    "1. **Input Space.** Collect training data: $\\{x_i,t_i\\}_{i=1}^N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Feature Space.** Extract polynomial features for some model order $M$: $\\phi(x_i) = [x_i^0,x_i^1,\\dots,x_i^M]^T, \\forall i=1,\\dots,N$\n",
    "    * We can create a **feature matrix**, $\\mathbf{X}$, where we stack each data point's feature representation in rows. $\\mathbf{X}$ is a $N\\times (M+1)$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Model Selection.** The linear regression model can be written in a linear functional form:\n",
    "$$\\mathbf{y} = f(\\mathbf{w},\\mathbf{x}) = w_0\\mathbf{x}^0 + w_1\\mathbf{x}^1 + \\dots + w_M\\mathbf{x}^M$$\n",
    "\n",
    "where $\\mathbf{w}=[w_0,w_1,\\dots,w_M]^T$ are unknown coefficients (or parameters) of the model.\n",
    "\n",
    "We can rewrite the model in matrix-vector notation:\n",
    "    \n",
    "$$\\mathbf{y} = f(\\mathbf{w},\\mathbf{x}) =\\mathbf{X}\\mathbf{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Objective (or cost or loss) Function.** Determine an objective function that allows us to evaluate the fitted model. We can use:\n",
    "$$\\text{Least Squares: } J(\\mathbf{w}) = \\sum_{i=1}^N e_i^2 = \\sum_{i=1}^N \\left(t_i-y_i\\right)^2$$\n",
    "$$\\text{Least Mean Squares (LMS): } J(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^N e_i^2 = \\frac{1}{N}\\sum_{i=1}^N \\left(t_i-y_i\\right)^2$$\n",
    "$$\\text{L1-norm: } J(\\mathbf{w}) = \\sum_{i=1}^N |e_i| = \\sum_{i=1}^N \\left|t_i-y_i\\right|$$\n",
    "$$\\text{and many others ...}$$ \n",
    "\n",
    "where $e_i = t_i - y_i$.\n",
    "    * The most common objective function is the LMS as it (1) will not depend on the number of samples, and (2) it is easy to take derivatives and find a closed-form solution for the parameters $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Learning Algorithm**. The learning algorithm is an *optimization algorithm*, it poses the problem as\n",
    "\n",
    "$$\\arg_{\\mathbf{w}} \\min J(\\mathbf{w},\\mathbf{x})$$\n",
    "\n",
    "It will *search* the objective function to find the *optimal* set of parameters $\\mathbf{w^*} = [w_0^*,w_1^*,\\dots,w_M^*]^T$ that **minimize** the objective function $J(\\mathbf{w})$. When using the Least Squares or the LMS objective function, the *optimal* solution for the parameters $\\mathbf{w}$ is given by:\n",
    "\n",
    "$$\\mathbf{w}^* = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we have just **trained the model**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(x,t,M):\n",
    "    X = np.array([x**m for m in range(M+1)]).T\n",
    "    w = np.linalg.inv(X.T@X)@X.T@t\n",
    "    y = X@w\n",
    "    error = t-y\n",
    "    return w, y, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = 8\n",
    "\n",
    "w, y, error = PolynomialRegression(x1,t1,M) \n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x1,y,'r', label = 'Estimated Polynomial', linewidth=5)\n",
    "plt.plot(x2,t2,'g', label = 'True Function', linewidth=5)\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x', fontsize=15)\n",
    "plt.ylabel('Desired Values, t', fontsize=15);\n",
    "\n",
    "print(np.sum(error**2)/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* What happens when the polynomial model order $M$ increases/decreases?\n",
    "\n",
    "* How large/small do the weight parameter values are as we increase $M$? Could this information be useful?\n",
    "\n",
    "* Which model order $M$ works best? Which $M$ would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have generated test data. In this synthetic environment, we have the label values for the test samples. But in practice, we will **not** have labels for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Stage**\n",
    "\n",
    "1. **Data acquisition.** In real world scenario, you will have samples coming in to the system for which the model will make a prediction for the desired response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Extract (same) features.** We will extract the same features as in the training stage, i.e., if we *fit* a model of polynomial order $M=3$, then we need to extract a 3rd-degree polynomial representation for every sample in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Predict Labels.** In this step, we will use the trained model (the vector of coefficients $\\mathbf{w}^*$ and compute the predicted labels for the feature representation of the test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = PolynomialRegression_test(x3, M, w)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x1,y,'r', label = 'Estimated Polynomial',linewidth=5)\n",
    "plt.plot(x2,t2,'g', label = 'True Function',linewidth=5)\n",
    "# plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x3,t3,'ro', label = 'Test Data')\n",
    "plt.plot(x3,y_test,'-c*', label = 'Test Predictions', linewidth=5)\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x', fontsize=15)\n",
    "plt.ylabel('Desired Values, t', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "* How did the trained model *fit* in the test data?\n",
    "* Is it able to *generalize*? What does *generalization* even mean?\n",
    "* Can we **design** a *training* strategy that can tells us how *well* we are performing in *unseen and unlabeled* data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Acquisition\n",
    "N = 40\n",
    "Ntest = 10 \n",
    "a, b = [0,1] \n",
    "gVar_train = 0.5 \n",
    "gVar_test = 1 \n",
    "x1, t1 = NoisySinusoidalData(N, a, b, gVar_train)    # training data and labels\n",
    "x2, t2 = NoisySinusoidalData(N, a, b, 0)             #true sine function\n",
    "x3, t3 = NoisySinusoidalData(Ntest, a, b, gVar_test) # test data and labels\n",
    "\n",
    "# Model parameters\n",
    "M = 4\n",
    "\n",
    "# Training and Test\n",
    "w, y, error = PolynomialRegression(x1,t1,M) \n",
    "y_test = PolynomialRegression_test(x3, M, w)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x3,t3,'ro', label = 'Test Data')\n",
    "plt.plot(x1,y,'r', label = 'Estimated Polynomial', linewidth=5)\n",
    "plt.plot(x2,t2,'g', label = 'True Function', linewidth=5)\n",
    "plt.plot(x3,y_test,'-c*', label = 'Test Predictions', linewidth=5)\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x', fontsize=15)\n",
    "plt.ylabel('Desired Values, t', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the **LMS** cost function as a function of the model order $M$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_train = []\n",
    "J_test = []\n",
    "Mrange=15\n",
    "\n",
    "for M in range(1,Mrange):\n",
    "    # to be finished in class\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.plot(list(range(1,Mrange)),J_train,'bo-', label = 'Training')\n",
    "plt.plot(list(range(1,Mrange)),J_test,'ro-', label = 'Test')\n",
    "plt.title('Cost Function')\n",
    "plt.legend()\n",
    "plt.xlabel('Model order, M')\n",
    "plt.ylabel('J(w)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "M = 13\n",
    "\n",
    "# Training and Test\n",
    "w, y, error = PolynomialRegression(x1,t1,M) \n",
    "y_test = PolynomialRegression_test(x3, M, w)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x3,t3,'ro', label = 'Test Data')\n",
    "plt.plot(x1,y,'r', label = 'Estimated Polynomial', linewidth=5)\n",
    "plt.plot(x2,t2,'g', label = 'True Function', linewidth=5)\n",
    "plt.plot(x3,y_test,'-c*', label = 'Test Predictions', linewidth=5)\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x', fontsize=15)\n",
    "plt.ylabel('Desired Values, t', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example\n",
    "\n",
    "Let's predict beer wet foam height after time of pour for three different beer brands. \n",
    "\n",
    "* The [beer foam data set](http://www.stat.ufl.edu/~winner/datasets.html) was collected by A. Leike and published in their work titled \"Demonstration of the Exponential Decay Law Using Beer Froth\" in 2002.\n",
    "\n",
    "**Goal:** use and optimize a linear regression that effectively *predicts* the wet foam height from different brands of beer after time of pour, using available information about that beer's previous foam height level at different times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Set Description\n",
    "\n",
    "The data contains measurements of wet foam height and beer height at various time points for 3 brands of beer. The author of this data set fit an *exponential decay model* of the form $H(t) = H_0 e^{-\\lambda t}$.\n",
    "\n",
    "The data set is saved as *.csv* file (**\"beer_foam.csv\"**) with information about the foam height (in cm) from 3 brands of beer over 15 measurement times (in seconds) after the time of pour.\n",
    "\n",
    "The file is organized in 4 columns:\n",
    "1. Time from pour (in seconds)\n",
    "2. Erdinger Weissbier foam height (in cm)\n",
    "3. Augustinerbrau Munchen foam height (in cm)\n",
    "4. Budweiser foam height (in cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.plot(data['Time'], data['Erdinger'], '-o', label='Erdinger')\n",
    "plt.plot(data['Time'], data['Augustinerbrau'], '-o', label='Augustinerbrau')\n",
    "plt.plot(data['Time'], data['Budweiser'], '-o', label='Budweiser')\n",
    "plt.legend()\n",
    "plt.xlabel('Time after pour, in seconds', fontsize=15)\n",
    "plt.ylabel('We Foam Height, in cm', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a **polynomial regression** model that predicts foam height for each brand of beer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the optimal set of parameters for the foam height model for each brand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 3\n",
    "\n",
    "w_erd, y_erd, e_erd = PolynomialRegression(x, t[:,0], M)\n",
    "w_aug, y_aug, e_aug = PolynomialRegression(x, t[:,1], M)\n",
    "w_bud, y_bud, e_bud = PolynomialRegression(x, t[:,2], M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficients for Erdinger model: w=',w_erd)\n",
    "print('Coefficients for Augustinerbrau model: w=',w_aug)\n",
    "print('Coefficients for Budweiser model: w=', w_bud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the predicted foam height at time $t=450$ seconds?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be finished in class\n",
    "\n",
    "print('At time t=450 s, the beer foam height predictions are:')\n",
    "print('Erdinger : ',erd_t450, ' cm')\n",
    "print('Augustinerbrau: ',aug_t450, ' cm')\n",
    "print('Budweiser: ', bud_t450, ' cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be completed in class...\n",
    "\n",
    "beers = ['Erdinger','Augustinerbrau','Budweiser']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for i in range(3):\n",
    "    plt.plot(x2,t2[:,i], '-o', label=beers[i])\n",
    "plt.legend()\n",
    "plt.xlabel('Time after pour, in seconds', fontsize=15)\n",
    "plt.ylabel('Wet Foam Height, in cm', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* Since we did not encode any type of physical behavior into the model (for example, enforcing the foam height to only decrease and to not take negative values), the model is not able to transcribe them.\n",
    "\n",
    "* The prediction point falls outside the region in which the model was trained and therefore the predictions are unreliable. For example, at a time t = 200 s, the model should do sufficiently well, because we have observations around that input value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erd_t200 = PolynomialRegression_test(200, M, w_erd)\n",
    "aug_t200 = PolynomialRegression_test(200, M, w_aug)\n",
    "bud_t200 = PolynomialRegression_test(200, M, w_bud)\n",
    "\n",
    "predictions = [erd_t200, aug_t200, bud_t200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,5))\n",
    "for i in range(3):\n",
    "    plt.plot(x, t[:,i],'-o',label=beers[i])\n",
    "    plt.plot(200, predictions[i],'xr', label=beers[i]+ ' prediction t=200s')\n",
    "plt.legend()\n",
    "plt.xlabel('Time, in seconds', fontsize=15)\n",
    "plt.ylabel('Foam height, in cm', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* The model choice also plays an important factor in the prediction. We know that the underlying model that characterizes foam height is an exponential model (as provided by the authors of this data set). \n",
    "\n",
    "* Let's try fitting a linear regression model over the logarithm transformation of the target value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have transformed the desired label as the log, our goal is to fit a linear model to approximate:\n",
    "\n",
    "\\begin{align}\n",
    "w_0 + w_1\\mathbf{x}+\\dots+w_M\\mathbf{x}^M &= \\ln(t) \\\\\n",
    "\\iff e^{w_0 + w_1\\mathbf{x}+\\dots+w_M\\mathbf{x}^M} &= t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1\n",
    "\n",
    "w_erd, y_erd, e_erd = PolynomialRegression(x, t_log[:,0], M)\n",
    "w_aug, y_aug, e_aug = PolynomialRegression(x, t_log[:,1], M)\n",
    "w_bud, y_bud, e_bud = PolynomialRegression(x, t_log[:,2], M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_erd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, np.exp(w_erd[0] + w_erd[1]*x),'-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,5))\n",
    "plt.plot(x,np.exp(w_erd[0] + w_erd[1]*x),'-o', label='Erdinger')\n",
    "plt.plot(x,np.exp(w_aug[0] + w_aug[1]*x),'-o', label='Augustinerbrau')\n",
    "plt.plot(x,np.exp(w_bud[0] + w_bud[1]*x),'-o', label='Budweiser')\n",
    "plt.legend()\n",
    "plt.xlabel('Time, in seconds', fontsize=15)\n",
    "plt.ylabel('Foam height, in cm', fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.hstack((x,[450]))\n",
    "\n",
    "fig=plt.figure(figsize=(8,5))\n",
    "plt.plot(x2,np.exp(w_erd[0] + w_erd[1]*x2),'-o', label='Erdinger')\n",
    "plt.plot(x2,np.exp(w_aug[0] + w_aug[1]*x2),'-o', label='Augustinerbrau')\n",
    "plt.plot(x2,np.exp(w_bud[0] + w_bud[1]*x2),'-o', label='Budweiser')\n",
    "plt.legend()\n",
    "plt.xlabel('Time, in seconds', fontsize=15)\n",
    "plt.ylabel('Foam height, in cm', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation Question:** From the example of beer foam height, how to you select which ML model setting should you use?\n",
    "\n",
    "* An ML model setting could simply be different parameter settings, such as different values for $M$ using the same model and features.\n",
    "* But in general, \"ML model setting\" refers to the collection of choices for all steps in the training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Experimental Design - How to use your data without cheating</b> \n",
    "\n",
    "In experimental design we need data to train (learn) models, and to test how good the models are. The training data needs to be different (disjoint) from the test data. Otherwise we would be testing the learned model on data it had previously seen, and we would get a biased estimate of the model's generalized performance.\n",
    "    \n",
    "Most machine learning algorithms require choosing parameter values; very often this is done by setting aside some of the training data to evaluate the quality of different parameter settings.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically we split the **training data** into three disjoint sets:\n",
    "* **Training set**, 80\\% - set of samples (and its labels) used to estimate the parameter values of the model (*learning the model*)\n",
    "* **Validation set** - set of samples (and its labels) used for exploring and picking best parameter values\n",
    "* **Test set**, typically 20\\% - set of samples (and its labels) used for testing the model generalization performance, and testing hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The key thing to remember when planning experiments is that the test data is used to form conclusions but not to make decisions during model building. Basing decisions on test data results is frequently called *cheating* in the machine learning community, and often results in wrong conclusions.\n",
    "\n",
    "2. Our generalization performance is only as good as our test set is representative of the true test data in application.\n",
    "\n",
    "3. After all parameter value decisions have been made, we often use **ALL** training data for the final training of the system and deployment.\n",
    "\n",
    "4. The training and validation sets may be rotated by using **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beer Foam Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the beer foam example from earlier. Let's split it into training and test sets (for now, I will omit the validation set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test = [i for j, i in enumerate(list(range(len(x)))) if j not in idx_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the test set to compare the performance of the between the polynomial regression and linear regression of transformed output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "M=3\n",
    "w_pol,_,_= PolynomialRegression(xtrain, ttrain, M)\n",
    "pred_pol = PolynomialRegression_test(xtest, M, w_pol)\n",
    "\n",
    "print('LMS: ',np.sum((ttest-pred_pol)**2)/len(ttest))\n",
    "print('Mean prediction error: ', np.mean(ttest-pred_pol))\n",
    "print('Median prediction error: ', np.median(ttest-pred_pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Exponential Regression\n",
    "\n",
    "M=3\n",
    "w_exp, _,_ = PolynomialRegression(xtrain, np.log(ttrain), M)\n",
    "pred_exp = np.exp(PolynomialRegression_test(xtest,M,w_exp))\n",
    "\n",
    "print('LMS: ',np.sum((ttest-pred_exp)**2)/len(ttest))\n",
    "print('Mean prediction error: ', np.mean(ttest-pred_exp))\n",
    "print('Median prediction error: ', np.median(ttest-pred_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens if we sample a new training set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot rely on one training run of the algorithm:\n",
    "* Variations in training/validation sets\n",
    "* random factors during training (e.g., random initialization, local optima, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>No Free Lunch Theorem</b> \n",
    "\n",
    "The No Free Lunch Theorem states that there is no single learning algorithm that in any domain always induces the most accurate learner. The usual approach is to try many and choose the one that performs the best on a separate validation set.\n",
    "    \n",
    "For any learning algorithm, there is a dataset where it is very accurate and another dataset where it is very poor. When we say that a learning algorithm is good, we only quantify how well its inductive bias matches the properties of the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of an algorithm can be determined using a variety of statistical measures. Some examples are: error rate, accuracy, ROC curves, performance-recall curves, etc.. But it can also be in terms of:\n",
    "* Risk,\n",
    "* Running time,\n",
    "* Training time and storage/memory,\n",
    "* Testing time and storage/memory,\n",
    "* Interpretability, namely, whether the method allows knowledge extraction which can be checked and validated by experts, and\n",
    "* computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors, Response, and Strategy of Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in other branches of science and engineering, in machine learning too, we do experiments to get information about the process under scrutiny.\n",
    "\n",
    "Our goal is to plan and conduct machine learning experiments and analyze the data resulting from the experiments, to be able to eliminate the effect of chance and obtain conclusions which we can consider *statistically significant*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of a trained learning system depends on:\n",
    "* **Controllable parameters:** hyper-parameters/settings of the algorithm/algorithm design choices\n",
    "\n",
    "* **Uncontrollable parameters:** noise in data, any randomness in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies of Experimentation\n",
    "\n",
    "To fully test a system, you want to try to evaluate each of these parameters separately. However, this is often not easily done.\n",
    "\n",
    "There are several *strategies of experimentation*:\n",
    "\n",
    "![Experimentation](figures/Experimentation.png)\n",
    "\n",
    "* Best guess\n",
    "* One factor at a time\n",
    "* Full/Partial Factorial design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principles of Experimental Design: Randomization, Replication, and Blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Randomization:** requires that the order in which the runs are carried out should be randomly determined so that the results are independent. For example, machines require some time to warm up until they operate in their normal range so tests should be done in random order for time not to bias the results.\n",
    "\n",
    "* **Replication:** for the same configuration of (controllable) factors, the experiment should be run a number of times to average over the effect of uncontrollable factors and induced randomization. In machine learning, this is typically done by running the same algorithm on a number of resampled versions of the same dataset; this is known as **cross-validation**, which we will discuss soon.\n",
    "\n",
    "* **Blocking:** is used to reduce or eliminate the variability due to nuisance factors that influence the response but in which we are not interested. For example, defects produced in a factory may also depend on the different batches of raw material, and this effect should be isolated from the controllable factors in the factory, such as the equipment, personnel, and so on. In ML experimentation, when we use resampling and use different subsets of the data for different replicates, we need to make sure that for example if we are comparing learning algorithms, they should all use the same set of resampled subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines for ML Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start experimentation, we need to have a good idea about what it is we are studying, how the data is to be collected, and how we are planning to analyze it.\n",
    "\n",
    "1. Understand the goal of the study\n",
    "2. Determine your evaluation metric(s)\n",
    "3. Determine what factors to vary and how to vary them\n",
    "4. Design your experiment (and get an estimate of how long it will take using a couple trial runs)\n",
    "5. Perform the experiment\n",
    "6. Analyze the result by performing statistical analysis\n",
    "7. Draw your conclusions based on your design and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
