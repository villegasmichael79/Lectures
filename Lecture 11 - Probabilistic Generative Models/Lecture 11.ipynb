{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11 - Probabilistic Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have focused on regression. We will begin to discuss **classification**.\n",
    "\n",
    "Suppose we have training data from two classes, $C_1$ and $C_2$, and we would like to train a classifier to assign a label to incoming test points whether they belong to class $C_1$ or $C_2$.\n",
    "\n",
    "There are *many* classifiers in the machine learning literature. We will cover a few in this course. Today we will focus on probabilistic generative approaches for classification.\n",
    "\n",
    "* There are two types of probabilistic models: **discriminative** and **generative**.\n",
    "\n",
    "A **discriminative** approach for classification is one in which we partition the feature space into regions for each class. Then, when we have a test point, we evaluate in which region it landed on and classify it accordingly.\n",
    "\n",
    "A **generative** approach for classification is one in which we estimate the parameters for distributions that generate the data for each class. Then, when we have a test point, we can compute the posterior probability of that point belonging to each class and assign the point to the class with the highest posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(mean1, mean2, cov1, cov2, N1, N2):\n",
    "    # We are generating data from two Gaussians to represent two classes\n",
    "    # In practice, we would not do this - we would just have data from the problem we are trying to understand\n",
    "    class1X = np.random.multivariate_normal(mean1, cov1, N1)\n",
    "    class2X = np.random.multivariate_normal(mean2, cov2, N2)\n",
    "    \n",
    "    plt.scatter(class1X[:,0], class1X[:,1], c='r')\n",
    "    plt.scatter(class2X[:,0], class2X[:,1])\n",
    "    plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "    plt.axis([-4,4,-4,4])\n",
    "    return class1X, class2X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = [-1, -1]\n",
    "mean2 = [1, 1]\n",
    "cov1 = [[1,0],[0,1]]\n",
    "cov2 = [[1,0],[0,1]]\n",
    "N1 = 50\n",
    "N2 = 100\n",
    "\n",
    "class1X, class2X = generateData(mean1, mean2, cov1, cov2, N1, N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data we generated above, we have a \"red\" class and a \"blue\" class. When we are given a test sample, we will want to assign the label of red or blue.\n",
    "\n",
    "We can compute the **posterior probability** for class $C_1$ as follows:\n",
    "\n",
    "$$P(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x)}$$\n",
    "\n",
    "Understanding that the two classes, red and blue, form a partition of all possible classes, then we can utilize the *Law of Total Probability*, and obtain:\n",
    "\n",
    "$$P(C_1|x)=\\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}$$\n",
    "\n",
    "Similarly, we can compute the posterior probability for class $C_2$:\n",
    "\n",
    "$$P(C_2|x) = \\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}$$\n",
    "\n",
    "Note that $P(C_1|x) + P(C_2|x) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Therefore, for a given test point $\\mathbf{x}^*$, our decision rule is:\n",
    "\n",
    "$$P(C_1|\\mathbf{x}^*) \\underset{C_2}{\\overset{C_1}{\\gtrless}} P(C_2|\\mathbf{x}^*)$$\n",
    "\n",
    "Using the Bayes' rule, we can further rewrite it as:\n",
    "\\begin{align}\n",
    "\\frac{P(\\mathbf{x}^*|C_1)P(C_1)}{P(\\mathbf{x}^*)} &\\underset{C_2}{\\overset{C_1}{\\gtrless}} \\frac{P(\\mathbf{x}^*|C_2)P(C_2)}{P(\\mathbf{x}^*)} \\\\\n",
    "\\iff P(\\mathbf{x}^*|C_1)P(C_1) &\\underset{C_2}{\\overset{C_1}{\\gtrless}} P(\\mathbf{x}^*|C_2)P(C_2)\n",
    "\\end{align}\n",
    "\n",
    "We assign $\\mathbf{x}^*$ as belonging to class 1 if $p(\\mathbf{x}^*|C_1) p(C_1) > p(\\mathbf{x}^*|C_2) p(C_2)$, or assign $\\mathbf{x}^*$ as belonging to class 2 if $p(\\mathbf{x}^*|C_1) p(C_1) < p(\\mathbf{x}^*|C_2) p(C_2)$.\n",
    "\n",
    "This defines the **Naive Bayes Classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Generative Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, **to train the classifier**, what we need to do is to determine the parametric forms and the associated parameters for $p(x|C_1)$, $p(x|C_2)$, $P(C_1)$ and $p(C_2)$.\n",
    "\n",
    "For example, we can assume that the data samples coming from either $C_1$ and $C_2$ are distributed according to Gaussian distributions. In this case, \n",
    "\n",
    "$$p(x|C_k) = \\frac{1}{(2\\pi)^{1/2} |\\Sigma_k|^{1/2}}\\exp\\left\\{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_k)^T\\Sigma_k^{-1}(\\mathbf{x}-\\mathbf{\\mu}_k)\\right\\}, \\forall k=\\{1,2\\}$$\n",
    "\n",
    "* We can consider any distributional form we want.\n",
    "\n",
    "What about the $P(C_1)$ and $P(C_2)$?\n",
    "\n",
    "* We can consider the relative frequency of each class, that is, $P(C_i) = \\frac{N_i}{N}$, where $N_i$ is the number of points in class $C_i$ and $N$ is the total number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Estimation\n",
    "\n",
    "For simplification, let's consider the covariance matrix $\\Sigma_k$ for $k=1,2$ to be **isotropic** matrices, that is, the covariance matrix is diagonal and the element along the diagonal is the same, or: $\\Sigma_k = \\sigma_k^2\\mathbf{I}$.\n",
    "\n",
    "* What are the parameters? The mean and covariance of the Gaussian distribution for both classes.\n",
    "\n",
    "Given the assumption of the Gaussian form, how would you estimate the parameters for $p(x|C_1)$ and $p(x|C_2)$? We can use **maximum likelihood estimate** for the mean and covariance, because we are looking for the parameters of the distributions that *maximize* the data likelihood!\n",
    "\n",
    "The MLE estimate for the mean of class $C_k$ is:\n",
    "\n",
    "$$\\mathbf{\\mu}_{k,\\text{MLE}} = \\frac{1}{N_k}\\sum_{n\\in C_k} \\mathbf{x}_n$$\n",
    "\n",
    "where $N_k$ is the number of training data points that belong to class $C_k$.\n",
    "\n",
    "To solve for the covariance of each class distribution $\\Sigma_k = \\sigma_k^2\\mathbf{I}$, we reduce to only finding one value, $\\sigma^2_k$. \n",
    "\n",
    "The MLE estimate for the variance $\\sigma^2_k is:\n",
    "\n",
    "$$\\sigma^2_{k,\\text{MLE}} = \\frac{1}{N_k}\\sum_{n\\in C_k} (\\mathbf{x}_n - \\mathbf{\\mu}_{k,\\text{MLE}}) (\\mathbf{x}_n - \\mathbf{\\mu}_{k,\\text{MLE}})^T$$\n",
    "\n",
    "(In practice, if we want to estimate an entire covariance matrix, we would have to take the derivative of the log-likelihood function with respect to every entry in the covariance matrix.)\n",
    "\n",
    "We can determine the values for $p(C_1)$ and $p(C_2)$ from the number of data points in each class:\n",
    "\n",
    "$$p(C_k) = \\frac{N_k}{N}$$\n",
    "\n",
    "where $N$ is the total number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the mean and covariance for each class from the training data\n",
    "\n",
    "# To be finished in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the prior for each class\n",
    "\n",
    "# To be finished in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a grid of values for x and y \n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = np.linspace(-4, 4, 100)\n",
    "xm, ym = np.meshgrid(x, y)\n",
    "X = np.dstack([xm,ym])\n",
    "\n",
    "# Let's plot the probabaility density function (pdf) for each class\n",
    "\n",
    "# to be finished in class\n",
    "\n",
    "plt.imshow(y1, extent=[-4,4,4,-4]); plt.xlabel('Feature 1'); plt.ylabel('Feature 2'); plt.title('pdf for Class 1'); plt.show()\n",
    "plt.imshow(y2, extent=[-4,4,4,-4]); plt.xlabel('Feature 1'); plt.ylabel('Feature 2'); plt.title('pdf for Class 2'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the posterior distributions: they represent our classification decision\n",
    "\n",
    "# to be finished in class\n",
    "\n",
    "plt.imshow(pos1, extent=[-4,4,4,-4]);plt.xlabel('Feature 1'); plt.ylabel('Feature 2'); plt.title('Posterior for Class 1')\n",
    "plt.show()\n",
    "plt.imshow(pos2, extent=[-4,4,4,-4]);plt.xlabel('Feature 1'); plt.ylabel('Feature 2'); plt.title('Posterior for Class 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the decision boundary:\n",
    "\n",
    "# to be finished in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
