{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 12 - Mixture Models & Expectation-Maximization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last class, we introduced the **Naive Bayes Classifier**, a type of probabilistic generative model.\n",
    "\n",
    "* In probabilistic generative models, we are interested in learning the posterior probability of each class in order to determine regions in our feature space that make each class more likely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Naive Bayes Classifier</b>\n",
    "    \n",
    "Decide class $C_k$ if:\n",
    "$$P(C_k|\\mathbf{x}) > P(C_j|\\mathbf{x}), \\forall j\\neq k$$\n",
    "\n",
    "we can rewrite it as:\n",
    "\n",
    "$$P(\\mathbf{x}|C_k)P(C_k) > P(\\mathbf{x}|C_j)P(C_j), \\forall j\\neq k$$\n",
    "    \n",
    "where $P(\\mathbf{x}|C_k)$ is the data likelihood of data samples in class $C_k$ and $P(C_k)$ is the prior probability of class $C_k$.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will consider the cases where the data likelihood of each class is better described as a **Mixture Model**. \n",
    "\n",
    "* To illustrate this, let's use our synthetic data generator function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generateData(mean1, mean2, cov1, cov2, N1, N2):\n",
    "    # We are generating data from two Gaussians to represent two classes\n",
    "    # In practice, we would not do this - we would just have data from the problem we are trying to understand\n",
    "    class1X = np.random.multivariate_normal(mean1, cov1, N1)\n",
    "    class2X = np.random.multivariate_normal(mean2, cov2, N2)\n",
    "    \n",
    "    plt.scatter(class1X[:,0], class1X[:,1], c='r')\n",
    "    plt.scatter(class2X[:,0], class2X[:,1])\n",
    "    plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "    plt.axis([-4,4,-4,4])\n",
    "    return class1X, class2X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = [-1, -1]\n",
    "mean2 = [1, 1]\n",
    "cov1 = [[1,0],[0,1]]\n",
    "cov2 = [[1,0],[0,1]]\n",
    "N1 = 50\n",
    "N2 = 100\n",
    "\n",
    "class1X, class2X = generateData(mean1, mean2, cov1, cov2, N1, N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to estimate the **data likelihood**, we can use the MLE estimates for the distribution parameters and fit a distributional form for the data likelihood.\n",
    "\n",
    "* Before we proceed with the parameter estimation, we need to assume a shape for the distribution. In practice, this selection is a *parameter* and needs to be validated using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Parameter Estimation Steps\n",
    "\n",
    "Assuming the classes follow a (bivariate or 2-D) Gaussian distribution and, for simplicity, let's assume the covariance matrices are **isotropic**, that is, $\\Sigma_k = \\sigma^2_k \\mathbf{I}$.\n",
    "\n",
    "The MLE steps for parameter estimation are:\n",
    "\n",
    "1. Write down the observed data likelihood, $\\mathcal{L}^0$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}^0 &= P(x_1,x_2,\\dots,x_N|C_k)\\\\\n",
    "&= \\prod_{n=1}^N P(x_n|C_k),\\text{ data samples are i.i.d.} \\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{(2\\pi)^{1/2} |\\Sigma|^{1/2}} \\exp\\left\\{-\\frac{1}{2}(x_n-\\mu_k)^T\\Sigma_k^{-1}(x_n-\\mu_k)\\right\\}\\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{(2\\pi)^{1/2} |\\sigma_k^2 \\mathbf{I}|^{1/2}} \\exp\\left\\{-\\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^T\\mathbf{I}(x_n-\\mu_k)\\right\\}\\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{(2\\pi)^{1/2} (\\sigma_k^2)^{1/2}} \\exp\\left\\{-\\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^T(x_n-\\mu_k)\\right\\}\n",
    "\\end{align}\n",
    "\n",
    "2. Take the log-likelihood, $\\mathbf{L}$. This *trick* helps in taking derivatives.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\ln\\left(\\mathcal{L}^0\\right) \\\\\n",
    "&= \\sum_{n=1}^N -\\frac{1}{2}\\ln 2\\pi - \\frac{1}{2}\\ln\\sigma_k^2 - \\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^T(x_n-\\mu_k)\n",
    "\\end{align}\n",
    "\n",
    "3. Take the derivative of the log-likelihood function with respect to the parameters of interest. For Gaussian distribution they are the mean and covariance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} &= 0\\\\\n",
    "\\sum_{n\\in C_k} \\frac{1}{\\sigma_k^2} (x_n - \\mu_k) &= 0\\\\\n",
    "\\sum_{n\\in C_k} (x_n - \\mu_k) &= 0 \\\\\n",
    "\\sum_{n\\in C_k} x_n - \\sum_{n\\in C_k} \\mu_k &= 0 \\\\\n",
    "\\sum_{n\\in C_k} x_n - N_k \\mu_k &= 0 \\\\\n",
    "\\mu_k & = \\frac{1}{N_k} \\sum_{n\\in C_k} x_n\n",
    "\\end{align}\n",
    "\n",
    "This is the sample mean for each class. And,\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\sigma_k^2} &= 0\\\\\n",
    "\\sum_{n\\in C_k} -\\frac{1}{2\\sigma_k^2} + \\frac{2(x_n - \\mu_k)^T(x_n - \\mu_k)}{(2\\sigma_k^2)^2} &=0 \\\\\n",
    "\\sum_{n\\in C_k} -1 + \\frac{(x_n - \\mu_k)^T(x_n - \\mu_k)}{\\sigma_k^2} &=0 \\\\\n",
    "\\sum_{n\\in C_k} \\frac{(x_n - \\mu_k)^T(x_n - \\mu_k)}{\\sigma_k^2} &=N_k \\\\\n",
    "\\sigma_k^2 &= \\sum_{n\\in C_k} \\frac{(x_n - \\mu_k)^T(x_n - \\mu_k)}{N_k}\n",
    "\\end{align}\n",
    "\n",
    "This is the sample variance for each class. Then we can create $\\Sigma_k = \\sigma_k^2 \\mathbf{I}$, which is the (biased) sample covariance for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Data Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = np.mean(class1X, axis=0)\n",
    "print('Mean of Class 1: ', mu1)\n",
    "\n",
    "cov1 = np.cov(class1X.T)\n",
    "print('Covariance of Class 1: ',cov1)\n",
    "\n",
    "mu2 = np.mean(class2X, axis=0)\n",
    "print('Mean of Class 2: ',mu2)\n",
    "\n",
    "cov2 = np.cov(class2X.T)\n",
    "print('Covariance of Class 2: ',cov2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a grid of values for x and y \n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = np.linspace(-4, 4, 100)\n",
    "xm, ym = np.meshgrid(x, y)\n",
    "X = np.dstack([xm,ym])\n",
    "\n",
    "# Let's plot the probabaility density function (pdf) for each class\n",
    "y1 = multivariate_normal.pdf(X, mean=mu1, cov=cov1) #P(x|C1)\n",
    "y2 = multivariate_normal.pdf(X, mean=mu2, cov=cov2) #P(x|C2)\n",
    "\n",
    "plt.imshow(y1, extent=[-4,4,4,-4]); plt.xlabel('Feature 1'); plt.ylabel('Feature 2'); plt.title('pdf for Class 1'); plt.show()\n",
    "plt.imshow(y2, extent=[-4,4,4,-4]); plt.xlabel('Feature 1'); plt.ylabel('Feature 2'); plt.title('pdf for Class 2'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we look at a relatively simple model where we model the each class group with a single Gaussian probability density function (pdf).\n",
    "\n",
    "* Consider the following example: would a multivariate Gaussian be able to represent this class group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1500\n",
    "groups = 5\n",
    "\n",
    "data, _ = make_blobs(n_samples = N, centers = groups)\n",
    "\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "plt.axis([-15,15,-15,15])\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we assume a single Gaussian distribution for this classes' data points, we would obtain a very poor estimate of the true underlying data likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = np.mean(data, axis=0)\n",
    "print('Mean of Class 1: ', mu1)\n",
    "\n",
    "cov1 = np.cov(data.T)\n",
    "print('Covariance of Class 1: ',cov1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a grid of values for x and y \n",
    "x = np.linspace(-15, 15, 100)\n",
    "y = np.linspace(-15,15, 100)\n",
    "xm, ym = np.meshgrid(x, y)\n",
    "X = np.dstack([xm,ym])\n",
    "\n",
    "y1 = multivariate_normal.pdf(X, mean=mu1, cov=cov1)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(y1, extent=[-15,15,15,-15])\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Models\n",
    "\n",
    "We can better represent this data with a **mixture model**:\n",
    "\n",
    "$$p(x|\\Theta) = \\sum_{k=1}^K \\pi_k P(x|\\Theta_k)$$\n",
    "\n",
    "where $\\Theta = \\{\\Theta_k\\}_{k=1}^K$ are set of parameters that define the distributional form in the probabilistic model $P(\\bullet|\\Theta_k)$ and \n",
    "\n",
    "\\begin{align}\n",
    "0 & \\leq \\pi_k \\leq 1\\\\\n",
    "& \\sum_k \\pi_k = 1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the probabilistic model $P(\\bullet|\\Theta_k)$ is assumed to be a Gaussian distribution, then the above mixture model is a **Gaussian Mixture Model (GMM)**\n",
    "\n",
    "$$P(x|\\Theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "where $\\Theta_k=\\{\\mu_k, \\Sigma_k, \\pi_k\\}, \\forall k$ are the mean, covariance and weight of each Gaussian distribution, and, again, $0 \\leq \\pi_k \\leq 1$ with $\\sum_k \\pi_k = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Exercise :</font> In code, how would you draw a sample from a Gaussian Mixture Model? from a mixture model in general?**\n",
    "\n",
    "* Note that in a Gaussian Mixture Model we are assuming that each data point $x_i$ was drawn from only one Gaussian. \n",
    "\n",
    "* Each data point $x_i$ has *hard membership*.\n",
    "\n",
    "* Each Gaussian in the Mixture Model will have its own $0 \\leq \\pi_k \\leq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate an event with arbitary probability $P_E$:\n",
    "\n",
    "1. Generate a random number R that is equally likely to be between 0 and 1.\n",
    "\n",
    "2. If $R \\le P_E$, then in the simulation, the event occurs. Otherwise it does not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example simulation\n",
    "Pe=0.13\n",
    "\n",
    "num_sims=100000\n",
    "event_count=0\n",
    "for sim in range(num_sims):\n",
    "    \n",
    "    # to be finished in class\n",
    "        \n",
    "print('According to the simulation Pe =~',event_count/num_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the case where we have 4 Gaussian in the Mixture model with weights $\\{0.4,0.25,0.25,0.1\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first (randomly) select a Gaussian and then draw a point from it.\n",
    "\n",
    "* How do you select from this set of Gaussians?\n",
    "\n",
    "* We can sum up the $\\pi$'s as we move from left to right and plot the running sums (or cumulative sum), then we can see sample a Gaussian using a Uniform random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_GaussianMixture(N, Means, Sigs, Pis):\n",
    "    X = np.empty((0,Means.shape[1]))\n",
    "    L = np.empty(0)\n",
    "    for i in range(N):\n",
    "        rv = npr.uniform()   # sample uniform RV\n",
    "        \n",
    "        # to be finished in class\n",
    "        \n",
    "    return X, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "Means = np.array([[1,1],[0,0],[-.5, 1],[.5, 1.5]])\n",
    "Sigs = [.1, .01, .05, .05]\n",
    "Pis = [.4, .25, .25, .1]\n",
    "\n",
    "X,L = make_GaussianMixture(N, Means, Sigs, Pis)\n",
    "\n",
    "fig = plt.figure(figsize=(14,6))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.scatter(X[:,0],X[:,1], c=L)\n",
    "plt.title('Synthetic Data with known Gaussians');\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.title('Gaussian Mixture - as received from the world');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Likelihood as a Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GMMs can be used to learn a complex distribution that represent a dataset. Thus, it can be used within the probabilistic generative classifier framework to model complex data likelihoods.\n",
    "\n",
    "* GMMs are also commonly used for **clustering**. Here a GMM is fit to a dataset with the goal of partitioning it into clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 1</font>**\n",
    "\n",
    "We can write the **observed data likelihood**, $\\mathcal{L}^o$, as:\n",
    "\n",
    "$$\\mathcal{L}^0 = \\prod_{i=1}^N p(x_i|\\Theta)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$p(x_i|\\Theta) = \\sum_{k=1}^K \\pi_k N(x_i|\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\Theta = \\{\\pi_k, \\mu_k, \\Sigma_k\\}_{k=1}^K$$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\\mathcal{L}^0 = \\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 2</font>**\n",
    "\n",
    "Let's apply our *trick*: log-likelihood.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\ln\\left(\\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\\right)\\\\\n",
    "&= \\sum_{i=1}^N \\ln \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 3</font>**\n",
    "\n",
    "Optimize for the parameters... \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} = 0, \\frac{\\partial \\mathcal{L}}{\\partial \\Sigma_k} = 0, \\text{ and }, \\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} = 0$$\n",
    "\n",
    "\n",
    "but this is a difficult problem to maximize!\n",
    "\n",
    "* A common approach for estimating the parameters of a GMM given a data set is by using the **Expectation-Maximization (EM) algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of (Gaussian) Mixture Models\n",
    "\n",
    "* **Data Representation and Inference**. Represent *any* data set using a complex distribution. You may want to describe your data with a distribution but the data does not fit into a standard form (e.g. Gamma, Gaussian, Exponential, etc.), then you can use a (Gaussian) Mixture Model to describe the data. Having a representation of your data in a distribution form is a very powerful tool that, other than having a model of the data, allows you infer about new samples, make predictions, etc.\n",
    "    * Having a parametric form for a real world model, allows us to apply it in simulation and use it for designing/optimizing decision-making solutions.\n",
    "\n",
    "* **Clustering.** Partition the data into groups. Note that in the GMMs formulation we did not add the concept of labels/targets. So GMMs are an **unsupervised learning** model. It represents the data with a very complex likelihood and then we can decompose that likelihood to partition the data into categories. This is also known as **clustering**.\n",
    "    * Commonly used in financial institutions to learn different group of individuals with specific characteristics of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization (EM) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the observation of i.i.d. samples $x_1, x_2, \\dots, x_N$ from the data likelihood $p(\\mathbf{x}|\\Theta)$ and we want to estimate the parameters (using MLE approach, for example)\n",
    "\n",
    "\\begin{align}\n",
    "& \\arg_{\\Theta} \\max p(\\mathbf{x}|\\Theta) \\\\\n",
    "= & \\arg_{\\Theta} \\max \\prod_{i=1}^N p(x_i|\\Theta) \n",
    "\\end{align}\n",
    "\n",
    "Now suppose the data samples $x_1, x_2, \\dots, x_N$ are **censored**.\n",
    "\n",
    "For example, suppose we observe i.i.d. samples, $x_1, x_2, \\dots, x_N$, from some sensor $f(\\mathbf{x})$. This sensor returns censored data in the form,\n",
    "\n",
    "\\begin{align}\n",
    "f(\\mathbf{x}) = \\begin{cases} x, &&\\text{ if }  x < a \\\\ a && \\text{ if }x\\geq a \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "This means that we see $x_1, x_2,\\dots, x_m$ (less than $a$) and we do not see $x_{m+1}, x_{m+2}, \\dots, x_N$ which are censored and set to $a$.\n",
    "\n",
    "* An example of such censored data would be a scale that can only measure weight up to 120 lbs. Any measurements above 120 would be *censored* at 120.\n",
    "\n",
    "* Given this censored data, we want to estimate the mean of the data as if the data was uncensored.\n",
    "\n",
    "For this case, we can write our *observed* data likelihood as:\n",
    "\n",
    "$$\\mathcal{L}^0 = \\prod_{i=1}^m p(x_i|\\Theta) \\prod_{j=m+1}^N \\int_a^{\\infty} p(x_j|\\Theta) dx_j$$\n",
    "\n",
    "The data likelihood would be very difficult to maximize to solve for $\\Theta$.\n",
    "\n",
    "If only we *knew* what the missing/censored data, the problem would be easy to solve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Latent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Expectation-Maximization** or **EM** algorithm is used to find the Maximum Likelihood Estimators (MLE) (or MAP estimators) for model parameters when data is incomplete, has missing data points, or has unobserved (hidden) latent variables (such as the case of censored data). \n",
    "\n",
    "* For all of these cases, the MLE optimization is very difficult to obtain by simply taking the derivative and solve for the parameters.\n",
    "\n",
    "**<font color=orange>Step 1</font>**\n",
    "The first step of EM is to characterized the observed likelihood $\\mathcal{L}^0$.\n",
    "\n",
    "**<font color=orange>Step 2</font>**\n",
    "Introduce *hidden latent variables* (also referred to as *hidden variables*) that simplify the observed data likelihood, $\\mathcal{L}$.\n",
    "\n",
    "**<font color=orange>Step 3</font>**\n",
    "Use the hidden variables to define the *complete likelihood* $\\mathcal{L}^c$.\n",
    "\n",
    "With this, we build an objective function:\n",
    "\n",
    "$$\\arg_{z,\\Theta} \\max E[\\ln(\\mathcal{L^c})|X,\\Theta^t]$$\n",
    "\n",
    "where $Q(\\Theta,\\Theta^t) = E[\\ln(\\mathcal{L^c})|X,\\Theta^t]$, $E[\\bullet]$ denotes expected value and $t$ denotes *iteration*. At $t=0$, we start with random values for the parameters $\\Theta$.\n",
    "\n",
    "Once we have this, the EM algorithm will alternate between E-step and M-step:\n",
    "\n",
    "1. **<font color=blue>E-step</font> (Expectation step)** Estimate the hidden variables. While holding $\\Theta$ fixed, find the variables $z$ that maximize $E[\\ln(\\mathcal{L^c})]$.\n",
    "\n",
    "2. **<font color=blue>M-step</font> (Maximization step)** Estimate the parameters of the complete data likelihood $\\mathcal{L}^c$. While holding the newly found variables $z$, find the best values for the parameters $\\Theta$ that maximize $E[\\ln(\\mathcal{L^c})]$.\n",
    "\n",
    "and it keeps iterating between E-step and M-step until convergence or until a certain number of iterations is reached.\n",
    "\n",
    "* EM is a general algorithm that can be applied to a variety of problems (not just the examples we are learning today). \n",
    "\n",
    "* EM is heavily tied with Maximum Likelihood Estimation (MLE). It is commonly used to simplify difficult MLE problems.\n",
    "    * It was originally introduced by Dempster, Laird, and Rubin in 1977 in a paper called \"Maximum Likelihood from Incomplete Data via the EM Algorithm\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Censored Data Example**\n",
    "\n",
    "Coming back to the censored data example, if we had observed the missing/censored data, we problem would be easy to solve. It would simplify to a standard MLE optimization.\n",
    "\n",
    "We can use EM in our censored data. For now, let's consider $\\mathbf{z}$ the hidden variables that represent the missing data. With this, we can write the *complete data likelihood* as:\n",
    "\n",
    "$$\\mathcal{L}^c = \\prod_{i=1}^m p(x_i|\\Theta) \\prod_{j=m+1}^N p(z_j|\\Theta)$$\n",
    "\n",
    "* Note that you cannot just use $a$ for the censored data, it would skew the results!\n",
    "\n",
    "The complete likelihood is much simpler to optimize, but then we need to define hidden variables $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing GMM with the EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed data likelihood for a Gaussian Mixture Model (GMM) is\n",
    "\n",
    "$$\\mathcal{L}^0 = \\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What hidden variables can we add to simplify this problem?**\n",
    "\n",
    "<!--* In this example, a hidden variable can be the label of the Gaussian from which $x_i$ was drawn from.\n",
    "\n",
    "$$z_i: \\text{label of the Gaussian from which $x_i$ was drawn from}$$\n",
    "\n",
    "If we know $z_i$, then for each data point we know which Gaussian it was drawn from along with its respective parameter $\\mu_{z_i}$ and $\\Sigma_{z_i}$ and its respective weight $\\pi_{z_i}$. So, each data point would had been drawn from $\\pi_{z_i}N(x_i|\\mu_{z_i}, \\Sigma_{z_i})$. \n",
    "\n",
    "Then, assuming we have $\\{z_i\\}_{i=1}^N$, we can write the complete data likelihood:\n",
    "\n",
    "$$\\mathcal{L}^c = \\prod_{i=1}^N \\pi_{z_i}\\mathcal{N}(x_i|\\mathbf{\\mu}_{z_i},\\Sigma_{z_i})$$ \n",
    "\n",
    "Now we can iterate between the **E-step** and **M-step** of the EM algorithm until we find convergence or we have reached a threshold for a number of iterations. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food for Thought\n",
    "\n",
    "In preparing for next class, think about how would you extend the objective function\n",
    "\n",
    "$$Q(\\Theta,\\Theta^t) = E[\\ln(\\mathcal{L^c})|X,\\Theta^t]$$\n",
    "\n",
    "using what you know about the expected value for continuous random variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
