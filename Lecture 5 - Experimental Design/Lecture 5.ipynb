{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 - Experimental Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(x,t,M):\n",
    "    X = np.array([x**m for m in range(M+1)]).T\n",
    "    w = np.linalg.inv(X.T@X)@X.T@t\n",
    "    y = X@w\n",
    "    error = t-y\n",
    "    return w, y, error\n",
    "\n",
    "def PolynomialRegression_test(x,M,w):\n",
    "    X = np.array([x**m for m in range(M+1)]).T \n",
    "    y = X@w\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('beer_foam.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.plot(data['Time'], data['Erdinger'], '-o', label='Erdinger')\n",
    "plt.plot(data['Time'], data['Augustinerbrau'], '-o', label='Augustinerbrau')\n",
    "plt.plot(data['Time'], data['Budweiser'], '-o', label='Budweiser')\n",
    "plt.legend()\n",
    "plt.xlabel('Time after pour, in seconds', fontsize=15)\n",
    "plt.ylabel('Wet Foam Height, in cm', fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['Time'].to_numpy() # input data\n",
    "\n",
    "t = data[['Erdinger','Augustinerbrau','Budweiser']].to_numpy() # desired labels, for each brand of beer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a polynomial regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 3\n",
    "\n",
    "w_erd, y_erd, e_erd = PolynomialRegression(x, t[:,0], M)\n",
    "w_aug, y_aug, e_aug = PolynomialRegression(x, t[:,1], M)\n",
    "w_bud, y_bud, e_bud = PolynomialRegression(x, t[:,2], M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erd_t450 = PolynomialRegression_test(450, M, w_erd)\n",
    "aug_t450 = PolynomialRegression_test(450, M, w_aug)\n",
    "bud_t450 = PolynomialRegression_test(450, M, w_bud)\n",
    "\n",
    "print('At time t=450 s, the beer foam height predictions are:')\n",
    "print('Erdinger : ',erd_t450, ' cm')\n",
    "print('Augustinerbrau: ',aug_t450, ' cm')\n",
    "print('Budweiser: ', bud_t450, ' cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.hstack((x,[450]))\n",
    "t2 = np.vstack((t, [erd_t450, aug_t450, bud_t450]))\n",
    "\n",
    "beers = ['Erdinger','Augustinerbrau','Budweiser']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for i in range(3):\n",
    "    plt.plot(x2,t2[:,i], '-o', label=beers[i])\n",
    "plt.legend()\n",
    "plt.xlabel('Time after pour, in seconds', fontsize=15)\n",
    "plt.ylabel('Wet Foam Height, in cm', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting an exponential model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have transformed the desired label as the log, our goal is to fit a linear model to approximate:\n",
    "\n",
    "\\begin{align}\n",
    "w_0 + w_1\\mathbf{x}+\\dots+w_M\\mathbf{x}^M &= \\ln(t) \\\\\n",
    "\\iff e^{w_0 + w_1\\mathbf{x}+\\dots+w_M\\mathbf{x}^M} &= t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_log = np.log(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 3\n",
    "\n",
    "w_erd, y_erd, e_erd = PolynomialRegression(x, t_log[:,0], M)\n",
    "w_aug, y_aug, e_aug = PolynomialRegression(x, t_log[:,1], M)\n",
    "w_bud, y_bud, e_bud = PolynomialRegression(x, t_log[:,2], M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erd_t450 = np.exp(PolynomialRegression_test(450, M, w_erd))\n",
    "aug_t450 = np.exp(PolynomialRegression_test(450, M, w_aug))\n",
    "bud_t450 = np.exp(PolynomialRegression_test(450, M, w_bud))\n",
    "\n",
    "print('At time t=450 s, the beer foam height predictions are:')\n",
    "print('Erdinger : ',erd_t450, ' cm')\n",
    "print('Augustinerbrau: ',aug_t450, ' cm')\n",
    "print('Budweiser: ', bud_t450, ' cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.hstack((x,[450]))\n",
    "t2 = np.vstack((t, [erd_t450, aug_t450, bud_t450]))\n",
    "\n",
    "beers = ['Erdinger','Augustinerbrau','Budweiser']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for i in range(3):\n",
    "    plt.plot(x2,t2[:,i], '-o', label=beers[i])\n",
    "plt.legend()\n",
    "plt.xlabel('Time after pour, in seconds', fontsize=15)\n",
    "plt.ylabel('Wet Foam Height, in cm', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation Question:** From the example of beer foam height, how to you select which ML model setting should you use?\n",
    "\n",
    "* An ML model setting could simply be different parameter settings, such as different values for $M$ using the same model and features.\n",
    "* But in general, \"ML model setting\" refers to the collection of choices for all steps in the training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Experimental Design - How to use your data without cheating</b> \n",
    "\n",
    "In experimental design we need data to train (learn) models, and to test how good the models are. The training data needs to be different (disjoint) from the test data. Otherwise we would be testing the learned model on data it had previously seen, and we would get a biased estimate of the model's generalized performance.\n",
    "    \n",
    "Most machine learning algorithms require choosing parameter values; very often this is done by setting aside some of the training data to evaluate the quality of different parameter settings.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically we split the **training data** into three disjoint sets:\n",
    "* **Training set**, 80\\% - set of samples (and its labels) used to estimate the parameter values of the model (*learning the model*)\n",
    "* **Validation set** - set of samples (and its labels) used for exploring and picking best parameter values\n",
    "* **Test set**, typically 20\\% - set of samples (and its labels) used for testing the model generalization performance, and testing hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The key thing to remember when planning experiments is that the test data is used to form conclusions but not to make decisions during model building. Basing decisions on test data results is frequently called *cheating* in the machine learning community, and often results in wrong conclusions.\n",
    "\n",
    "2. Our generalization performance is only as good as our test set is representative of the true test data in application.\n",
    "\n",
    "3. After all parameter value decisions have been made, we often use **ALL** training data for the final training of the system and deployment.\n",
    "\n",
    "4. The training and validation sets may be rotated by using **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beer Foam Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the beer foam example from earlier. Let's split it into training and test sets (for now, I will omit the validation set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test = [i for j, i in enumerate(list(range(len(x)))) if j not in idx_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the test set to compare the performance of the between the polynomial regression and linear regression of transformed output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "M=3\n",
    "w_pol,_,_= PolynomialRegression(xtrain, ttrain, M)\n",
    "pred_pol = PolynomialRegression_test(xtest, M, w_pol)\n",
    "\n",
    "print('LMS: ',np.sum((ttest-pred_pol)**2)/len(ttest))\n",
    "print('Mean prediction error: ', np.mean(ttest-pred_pol))\n",
    "print('Median prediction error: ', np.median(ttest-pred_pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Exponential Regression\n",
    "\n",
    "M=3\n",
    "w_exp, _,_ = PolynomialRegression(xtrain, np.log(ttrain), M)\n",
    "pred_exp = np.exp(PolynomialRegression_test(xtest,M,w_exp))\n",
    "\n",
    "print('LMS: ',np.sum((ttest-pred_exp)**2)/len(ttest))\n",
    "print('Mean prediction error: ', np.mean(ttest-pred_exp))\n",
    "print('Median prediction error: ', np.median(ttest-pred_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens if we sample a new training set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot rely on one training run of the algorithm:\n",
    "* Variations in training/validation sets\n",
    "* random factors during training (e.g., random initialization, local optima, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>No Free Lunch Theorem</b> \n",
    "\n",
    "The No Free Lunch Theorem states that there is no single learning algorithm that in any domain always induces the most accurate learner. The usual approach is to try many and choose the one that performs the best on a separate validation set.\n",
    "    \n",
    "For any learning algorithm, there is a dataset where it is very accurate and another dataset where it is very poor. When we say that a learning algorithm is good, we only quantify how well its inductive bias matches the properties of the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of an algorithm can be determined using a variety of statistical measures. Some examples are: error rate, accuracy, ROC curves, performance-recall curves, etc.. But it can also be in terms of:\n",
    "* Risk,\n",
    "* Running time,\n",
    "* Training time and storage/memory,\n",
    "* Testing time and storage/memory,\n",
    "* Interpretability, namely, whether the method allows knowledge extraction which can be checked and validated by experts, and\n",
    "* computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors, Response, and Strategy of Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in other branches of science and engineering, in machine learning too, we do experiments to get information about the process under scrutiny.\n",
    "\n",
    "Our goal is to plan and conduct machine learning experiments and analyze the data resulting from the experiments, to be able to eliminate the effect of chance and obtain conclusions which we can consider *statistically significant*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of a trained learning system depends on:\n",
    "* **Controllable parameters:** hyper-parameters/settings of the algorithm/algorithm design choices\n",
    "\n",
    "* **Uncontrollable parameters:** noise in data, any randomness in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies of Experimentation\n",
    "\n",
    "To fully test a system, you want to try to evaluate each of these parameters separately. However, this is often not easily done.\n",
    "\n",
    "There are several *strategies of experimentation*:\n",
    "\n",
    "![Experimentation](figures/Experimentation.png)\n",
    "\n",
    "* Best guess\n",
    "* One factor at a time\n",
    "* Full/Partial Factorial design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principles of Experimental Design: Randomization, Replication, and Blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Randomization:** requires that the order in which the runs are carried out should be randomly determined so that the results are independent. For example, machines require some time to warm up until they operate in their normal range so tests should be done in random order for time not to bias the results.\n",
    "\n",
    "* **Replication:** for the same configuration of (controllable) factors, the experiment should be run a number of times to average over the effect of uncontrollable factors and induced randomization. In machine learning, this is typically done by running the same algorithm on a number of resampled versions of the same dataset; this is known as **cross-validation**, which we will discuss soon.\n",
    "\n",
    "* **Blocking:** is used to reduce or eliminate the variability due to nuisance factors that influence the response but in which we are not interested. For example, defects produced in a factory may also depend on the different batches of raw material, and this effect should be isolated from the controllable factors in the factory, such as the equipment, personnel, and so on. In ML experimentation, when we use resampling and use different subsets of the data for different replicates, we need to make sure that for example if we are comparing learning algorithms, they should all use the same set of resampled subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines for ML Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start experimentation, we need to have a good idea about what it is we are studying, how the data is to be collected, and how we are planning to analyze it.\n",
    "\n",
    "1. Understand the goal of the study\n",
    "2. Determine your evaluation metric(s)\n",
    "3. Determine what factors to vary and how to vary them\n",
    "4. Design your experiment (and get an estimate of how long it will take using a couple trial runs)\n",
    "5. Perform the experiment\n",
    "6. Analyze the result by performing statistical analysis\n",
    "7. Draw your conclusions based on your design and result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Regression Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our synthetic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisySinusoidalData(N, a, b, gVar):\n",
    "    '''NoisySinusoidalData(N, a, b, gVar): Generates N data points in the range [a,b] \n",
    "    sampled from a sin(2*pi*x) with additive zero-mean Gaussian random noise with standard deviation gVar'''\n",
    "    x = np.linspace(a,b,N) # N input samples, evenly spaced numbers between [a,b]\n",
    "    noise = npr.normal(0,gVar,N) # draw N sampled from a univariate Gaussian distribution with mean 0, gVar standard deviation and N data points\n",
    "    t = np.sin(2*np.pi*x) + noise # desired values, noisy sinusoidal \n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input samples and desired values\n",
    "\n",
    "N = 40 # number of training data samples\n",
    "Ntest = 10 # number test data samples\n",
    "a, b = [0,1] # data samples interval\n",
    "gVar_train = 0.5 # standard deviation of the zero-mean Gaussian noise observed in the training samples\n",
    "gVar_test = 1 # standard deviation of the zero-mean Gaussian noise observed in the testing samples\n",
    "x1, t1 = NoisySinusoidalData(N, a, b, gVar_train) # Training Data - Noisy sinusoidal\n",
    "x2, t2 = NoisySinusoidalData(N, a, b, 0) # True Sinusoidal - in practice, we don't the true function\n",
    "x3, t3 = NoisySinusoidalData(Ntest, a, b, gVar_test) # Test Data - Noisy sinusoidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x1, t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x2, t2, 'g', label = 'True Sinusoidal')\n",
    "plt.plot(x3, t3, 'ro', label = 'Test Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x')\n",
    "plt.ylabel('Desired Values, t');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Order\n",
    "M = 3\n",
    "\n",
    "# Find the parameters that fit the noisy sinusoidal\n",
    "w, y, error = PolynomialRegression(x1,t1,M) \n",
    "\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x1,y,'r', label = 'Estimated Polynomial')\n",
    "plt.plot(x2,t2,'g', label = 'True Function')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Data Samples, x\")\n",
    "plt.ylabel(\"Desired Values, t\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How would you report these results?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the overall error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure. We can also compute many other statistics such as mean error, median error, maximum, etc.\n",
    "\n",
    "But is this number useful? Is it informing us in deciding whether this model is a good fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform **goodness of fit**, in regression, we note that we are modeling a continuous-valued function that approximates some *unknown* continuous-valued function. And because we do not have the full functions **and** the data is noisy, it is simpler to characterize *both* the predictions and observational data as random variables.\n",
    "\n",
    "A common statistic for compute goodness of fit in regression is:\n",
    "\n",
    "* coefficient of determination (or $r^2$, r-squared) of the Quantile-Quantile (Q-Q) plot\n",
    "    * the $r^2$ is the squared value of the Pearson's correlation\n",
    "* Mean-difference hypothesis test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The **Quantile-Quantile plot** measures the one-to-one correspondence of all quantiles of the random variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete in class\n",
    "plt.xlabel('True Labels',fontsize=15)\n",
    "plt.ylabel('Predictions', fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical values considered acceptable $r^2>90\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The mean-difference hypothesis test essentially poses the two hypothesis:\n",
    "\n",
    "$H_0$: the predicted values come from the same distribution as the target values\n",
    "$H_1$: the predicted values do not come from the same distribution\n",
    "\n",
    "If we consider a significance level of $\\alpha=0.05$, can we accept the null hypothesis?\n",
    "\n",
    "* Let's build a simulation for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The *p-value* is larger than the significance level $\\alpha$, therefore we cannot reject the null hypothesis. The data indicates that the predicted values have the same distribution as the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
