{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7 - The Bias-Variance Trade-Off; Bayesian Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n",
    "from IPython.display import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(x,t,M):\n",
    "    X = np.array([x**m for m in range(M+1)]).T\n",
    "    w = np.linalg.inv(X.T@X)@X.T@t\n",
    "    y = X@w\n",
    "    error = t-y\n",
    "    return w, y, error\n",
    "\n",
    "def PolynomialRegression_test(x,M,w):\n",
    "    X = np.array([x**m for m in range(M+1)]).T \n",
    "    y = X@w\n",
    "    return y\n",
    "\n",
    "def NoisySinusoidalData(N, a, b, gVar):\n",
    "    x = np.linspace(a,b,N)\n",
    "    noise = npr.normal(0,gVar,N) \n",
    "    t = np.sin(2*np.pi*x) + noise\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Regularized Least Squares</b> \n",
    "\n",
    "We fit the linear regression model such that the *regularized* objective function $J(\\mathbf{w})$ is minimized:\n",
    "\n",
    "$$\\arg_{\\mathbf{w}}\\min J(\\mathbf{w})$$\n",
    "    \n",
    "where   \n",
    "    \n",
    "\\begin{align} \n",
    "J(\\mathbf{w}) &= J_D(\\mathbf{w}) + \\lambda J_W(\\mathbf{w}) \\\\\n",
    "&= \\frac{1}{2} \\left\\Vert \\mathbf{t} - \\mathbf{X}\\mathbf{w} \\right\\Vert^2_2 + \\frac{\\lambda}{2}\\left\\Vert\\mathbf{w}\\right\\Vert_2^2\n",
    "\\end{align}\n",
    "\n",
    "This is often referred to as the **ridge regression**. The optimal solution $\\mathbf{w}^*$ is:\n",
    "\n",
    "$$\\mathbf{w}^* = \\left(\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I}\\right)^{-1}\\mathbf{X}^T\\mathbf{t} $$\n",
    "\n",
    "where $\\mathbf{I}$ is an identity matrix of size $(M+1)\\times (M+1)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are **diagonally loading** the matrix $\\mathbf{X}^T\\mathbf{X}$ with the regularizer term $\\lambda$.\n",
    "* This is \"filling\" the feature space such that the matrix $\\mathbf{X}^T\\mathbf{X}$ becomes full rank.\n",
    "* What happens when $\\lambda \\rightarrow \\infty$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In code, how would you change the function ```PolynomialRegression``` created above to include the regularization term?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression_reg(x,t,M,lam):\n",
    "    \n",
    "    # to complete in class\n",
    "    \n",
    "    return w, y, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "Ntest = 20 \n",
    "a, b = [0,1] \n",
    "gVar_train = 1\n",
    "gVar_test = 0.5 \n",
    "x1, t1 = NoisySinusoidalData(N, a, b, gVar_train) \n",
    "x2, t2 = NoisySinusoidalData(N, a, b, 0)\n",
    "x3, t3 = NoisySinusoidalData(Ntest, a, b, gVar_test)\n",
    "\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x2,t2,'g', label = 'True Function')\n",
    "plt.plot(x3,t3,'ro', label = 'Test Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x')\n",
    "plt.ylabel('Desired Values, t');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "lam = 0.000001\n",
    "\n",
    "_, y, _ = PolynomialRegression(x1,t1,M) \n",
    "_, yreg, _ = PolynomialRegression_reg(x1,t1,M,lam) \n",
    "\n",
    "fig=plt.figure(figsize=(10,8))\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x1,y,'r',linewidth=5, label = 'Estimated Polynomial')\n",
    "plt.plot(x1,yreg,'c',linewidth=5, label = 'Estimated Polynomial w/ Reg.')\n",
    "plt.plot(x2,t2,'g',linewidth=5, label = 'True Function')\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('Data Samples, x', fontsize=20)\n",
    "plt.ylabel('Desired Values, t', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are the parameters we can control now? <!--Model order $M$ and regularizer weight $\\lambda$-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>Important:</font> In practice, when computing $(\\mathbf{X}^T\\mathbf{X})^{-1}$, if you receive an *error: matrix is singular*, what do you do?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis Functions\n",
    "\n",
    "So far, we have assumed that we have form a **feature matrix** $\\mathbf{X}$ of dimensions $N \\times (M+1)$, where $N$ is the number of samples and $M$ the number of coefficients.\n",
    "\n",
    "* In Polynomial Regression, we constructed this matrix with a polynomial representation of each data sample:\n",
    "\n",
    "$$\\phi(x_i) = [x_i^0, x_i^1, \\cdots, x_i^M]^T$$\n",
    "\n",
    "In practice, we can use other types of features. But, from a software implementation point-of-view, regardless of the features used, we always want to have them in a tidy feature matrix $\\mathbf{X}$:\n",
    "\n",
    "$$\\mathbf{X} =\\left[\\begin{array}{c} \\phi(x_1)^T \\\\ \\phi(x_2)^T \\\\ \\vdots \\\\ \\phi(x_N)^T \\end{array}\\right]  = \\left[\\begin{array}{ccccc}\n",
    "1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{M}\\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{M}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{M}\n",
    "\\end{array}\\right] \\in \\mathbb{R}^{N\\times (M+1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In mathematics, the polynomial feature representation that we have used is called a **polynomial basis function** and is a basis of a polynomial ring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also implement the **Linear Regression** model using other feature representations, also called **basis functions** $\\phi(\\mathbf{x})$. \n",
    "\n",
    "    * **Assumption:** we are assuming that true *unknown* function $f(x)$ can be modeled by at least one of the functions $\\phi(\\mathbf{x})$ that can be represented by a linear combination of the basis functions, i.e., by one function in the function class under consideration.\n",
    "    \n",
    "    * If we include **too few** basis functions or unsuitable basis functions, we might not be able to model the true dependency. Similarly to polynomial features, the more we added, the better fit we have to out training data.\n",
    "    \n",
    "    * If we include **too many** basis functions, we need many data points to fit all the unknown parameters.\n",
    "        * There are special function space (Reproducing Kernel Hilbert Space or RKHS) where we can have an *infinite* number of basis functions (called kernels) and still have good generalization.\n",
    "    \n",
    "    * What we control? Which basis functions to use, how many to use, and any other parameters they have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples include:\n",
    "\n",
    "1. **Radial Basis Functions**\n",
    "\n",
    "Another class of basis functions are radial basis functions (RBF). Typical representatives are Gaussian basis functions:\n",
    "\n",
    "$$\\phi_j(x) = \\exp\\left\\{-\\frac{(x-\\mu_j)^2)}{2s_j^2}\\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "m = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "s = [0.05]*10\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x2,t2,'g',linewidth=5, label = 'True Function')\n",
    "for i in range(M):\n",
    "    plt.plot(x1, RBF(x1,m[i],s[i]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these basis functions, we need to:\n",
    "1. compute the weight value that each one has in representing the underlying function $f(x)$\n",
    "2. estimate the parameters of each basis function, in particular, the mean $\\mu$ and standard deviation $\\sigma$ for each Gaussian function.\n",
    "\n",
    "We will see exactly how to do this when we study **Gaussian Mixture Models (GMMs)**, but we are not quite ready yet..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Fourier Basis functions**\n",
    "\n",
    "3. **Wavelet Basis functions**\n",
    "\n",
    "4. and many others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the form and number of the basis functions, we still face the problem of **overfitting**. In fact, this is true for any ML model.\n",
    "\n",
    "The problem of searching for *optimal* model complexity has been extensively studied; in fact, we are *simply* searching for a level of complexity that fits the data *well* yet *not too well*. This phenomenon can be summarized by the **bias-variance trade-off** of complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>The Bias-Variance Trade-Off</b> \n",
    "\n",
    "The bias-variance trade-off is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('figures/BiasvsVariance.png',width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the cost function:\n",
    "\n",
    "$$J(\\mathbf{x},\\mathbf{w}) = \\frac{1}{2} \\Vert \\mathbf{t} - \\mathbf{y}\\Vert_2^2$$\n",
    "\n",
    "where $\\mathbf{y} = \\sum_{j=0}^{M} w_j\\phi_j(\\mathbf{x})$\n",
    "\n",
    "We can model each $J$ as a random variable, and so its expected value, over some data set $D$, is given by:\n",
    "\n",
    "\\begin{align}\n",
    "E\\left[J(\\mathbf{x},\\mathbf{w})\\right] &= E\\left[\\frac{1}{2} \\Vert \\mathbf{t} - \\mathbf{y}\\Vert_2^2\\right] \\\\\n",
    "&=E\\left[(\\mathbf{t} - \\mathbf{y})^2\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{y}$ is the model and $\\mathbf{t}$ is the desired response. For the experimental design section, we learned that this quantity dependent on the particular data set $D$. And so, we can take its average over the ensemble of data sets. If we add and subtract the quantity $E_D[\\mathbf{y}]$.\n",
    "\n",
    "* In practice we really only have access to $E_D[\\mathbf{y}]$\n",
    "* But $\\mathbf{y}$ here, is simply representing the model *if* we had an *infinite* amount of data and could effectively represent it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "E\\left[(\\mathbf{t} - \\mathbf{y})^2\\right] &= E\\left[(\\mathbf{t} - E_D[\\mathbf{y}] + E_D[\\mathbf{y}] - \\mathbf{y})^2\\right]\\\\\n",
    "&= E\\left[(\\mathbf{t} - E_D[\\mathbf{y}])^2\\right] + E\\left[(E_D[\\mathbf{y}] - \\mathbf{y})^2\\right] + E\\left[2(\\mathbf{t} - E_D[\\mathbf{y}])(E_D[\\mathbf{y}] - \\mathbf{y})\\right]\\\\\n",
    "&= \\text{variance } +  \\text{bias } +  \\text{irreducible error} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Interpretation - The Evidence Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the **Regularized Least Squares** in the \"Objective Function world\", where we simply add a term to our objective in order to prevent overfitting and, consequently, allow the model to generalize to unseen and unkown data.\n",
    "\n",
    "Another way to look at Regularized Least Squares is from a Bayesian point-of-view. To see this, let's look at our objective function:\n",
    "\n",
    "\\begin{align}\n",
    "& \\arg_{\\mathbf{w}}\\min \\left(J(\\mathbf{w})\\right) \\\\\n",
    "= & \\arg_{\\mathbf{w}}\\max \\left(- J(\\mathbf{w})\\right) \\\\\n",
    "= & \\arg_{\\mathbf{w}}\\max \\left(\\exp\\left(- J(\\mathbf{w})\\right)\\right) \\text{, }\\exp(\\bullet)\\text{ is a monotonic function}  \n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "$$J(\\mathbf{w})= \\frac{1}{2}\\sum_{n=1}^N \\left(t_n - y_n\\right)^2 - \\frac{\\lambda}{2} \\sum_{i=0}^M w_i^2$$\n",
    "and, consider e.g. the polynomial model (this could be *any* model)\n",
    "$$y_n = \\sum_{j=0}^M w_jx_n^j$$\n",
    "\n",
    "Then,\n",
    "\n",
    "\\begin{align}\n",
    "& \\arg_{\\mathbf{w}}\\max \\left(\\exp\\left(-\\frac{1}{2}\\sum_{n=1}^N \\left(t_n - y_n\\right)^2 - \\frac{\\lambda}{2} \\sum_{i=0}^M w_i^2)\\right)\\right) \\\\\n",
    "= & \\arg_{\\mathbf{w}}\\max \\left(\\exp\\left(-\\frac{1}{2}\\sum_{n=1}^N \\left(t_n - y_n\\right)^2\\right) \\exp\\left(- \\frac{\\lambda}{2} \\sum_{i=0}^M w_i^2)\\right)\\right) \\\\\n",
    "=& \\arg_{\\mathbf{w}}\\max \\left(\\prod_{n=1}^N \\exp\\left(-\\frac{1}{2}\\left(t_n - y_n\\right)^2\\right) \\prod_{i=0}^M \\exp \\left(-\\frac{\\lambda}{2} w_i^2\\right) \\right)\\text{, assuming the data }\\{(x_n,t_n)\\}_{n=1}^N\\text{ is i.i.d.}  \\\\\n",
    "\\approx & \\arg_{\\mathbf{w}}\\max \\mathcal{N}\\left(\\mathbf{t}| \\mathbf{y}, 1\\right) \\mathcal{N}\\left(0, 1/\\lambda\\right) \\\\\n",
    "=& \\arg_{\\mathbf{w}}\\max p(\\mathbf{t}|\\mathbf{w}) p(\\mathbf{w}) \\\\\n",
    "=& \\arg_{\\mathbf{w}}\\max p(\\mathbf{w}|\\mathbf{t}) p(\\mathbf{t}), \\text{ using Bayes' Rule} \\\\\n",
    "\\propto & \\arg_{\\mathbf{w}}\\max p(\\mathbf{w}|\\mathbf{t}), p(\\mathbf{t})\\text{ is constant for some fixed training set}  \n",
    "\\end{align}\n",
    "\n",
    "where $p(\\mathbf{t}|\\mathbf{w})$ is known as the **data likelihood**, $p(\\mathbf{w})$ is known as the **prior** on the parameters, and $p(\\mathbf{w}|\\mathbf{t})$ is the **posterior probability**.\n",
    "\n",
    "In Machine Learning, this result is known as the **evidence approximation**.\n",
    "\n",
    "* In practice, this means that we now can rewrite the Regularized Least Squares problem as the product between the *data likelihood* and a *prior distribution* on the parameters. \n",
    "\n",
    "    * In particular, for Least Squares cost function and an L2- regularization term, both distributions (likelihood's and prior's) follow a Gaussian distribution.\n",
    "    \n",
    "* Now, we can select **any** distribution function to our data and control the regularization also using a probabilistic model!\n",
    "\n",
    "* **What is the shape of the prior distribution if we had considered the L1-norm or the Lasso regularizer?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
