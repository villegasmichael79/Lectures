{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exam will focus on conceptual and algorithmic questions, and will assess material covered from Lecture 1 to Lecture 17.\n",
    "\n",
    "**<font color=blue>Allowed Material:</font>**\n",
    "* You are allowed 1-page letter-sized front and back of **formulas** (handwritten or typed).\n",
    "    * Other than labeling your equations, **DO NOT** write down definitions, pseudo-code or any algorithm description\n",
    "* You are allowed a scientific calculator, but will probably not need it\n",
    "* Bring clean paper to write your answers (go ahead and get extra, just in case)\n",
    "    * Some questions will be typed directly in the quiz, others will be solved on paper\n",
    "* Bring a second device (e.g., phone, tablet or other) with the [CamScanner](https://www.camscanner.com/) or [Scannable](https://evernote.com/products/scannable) app installed. This device is only to be used at the end of the exam to take pictures of your **handwritten solutions AND formula sheet**.\n",
    "* **<font color=orange>TOTAL TIME:</font>** 2 hours + 15 minutes\n",
    "\n",
    "**<font color=red>Communications between students or anyone else is considered cheating. Turn off all Slack notifications and other communications channels!</font>**\n",
    " \n",
    "Ten (10) minutes prior to the time you are scheduled to take the exam, you will be able to see the Exam quiz in Canvas.\n",
    "\n",
    "Make sure your testing environment is comfortable, and start whenever you are ready.\n",
    "\n",
    "The total allocated time 2 hours + 15 minutes, the \"clock\" starts counting when you begin to take the quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 18 - Exam 1 Review Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for posting your questions!\n",
    "\n",
    "I will be answering the ones with the most likes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post 1\n",
    "\n",
    "**I had a lot of trouble with some of the questions of Homework 2 part 1, so covering the questions on that would be helpful.**\n",
    "\n",
    "**The Bayesian interpretation question and the k-means question are two that come to mind.**\n",
    "\n",
    "## Post 8\n",
    "\n",
    "**Could you also go over the last problem in Hw2 part 1 please?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think both these questions can be answered simultaneously. If anything is left out, let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the questions here: https://github.com/Fundamentals-of-Machine-Learning-F20/Assignment-Solutions, and discuss the solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post 2\n",
    "\n",
    "**I don't necessarily have any specific content questions but can we rehash basically how to prepare for the exam, i.e, what content is most useful to study whats least useful as well the format of the exam?**\n",
    "\n",
    "## Post 3\n",
    "\n",
    "**I think that a short overview of all the algorithms that we should know would be extremely helpful. Having a one-stop lecture to assess how well we know everything would be really helpful for knowing areas of studying that we need to focus on. I also struggled with HW2 part 3. I think an overview of that question would be very beneficial as well. Thank you.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think both these questions can be answered simultaneously. If anything is left out, let me know!\n",
    "* Let's take a look at the list below and discuss the material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All material covered in the lecture notes and homework assignments is useful. All the questions will be designed based on what we learned and discussed in class. \n",
    "\n",
    "To better prepare for the midterm: read, review and redo all the derivations presented in the lecture notes and assignment solutions (at least Part 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a highlight of the topics to be assessed in the Midterm exam:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic 1: What is Machine Learning?** Lectures 1-2\n",
    "\n",
    "* Define and differentiate ML from Deep Learning and Artificial Intelligence.\n",
    "* Design at least one example for each type\n",
    "* How do we define *learning* in Machine Learning?\n",
    "* Design the flowchart for supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic 2: Regression** Lectures 3-5\n",
    "* What type of learning does regression use?\n",
    "* What is regression (and how is it different from e.g. classification and clustering)?\n",
    "* Define the linear regression model with basis functions.\n",
    "* What is the Least Squares solutions for regression with and without regularization term?\n",
    "* Derive the solution for the polynomial regression, and basis function regression model.\n",
    "* Name at least one application of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic 3: Experimental Design** Lectures 5-6\n",
    "* Why experimental design important?\n",
    "* What are the steps one takes to *design an experiment* in Machine Learning?\n",
    "* How do you use k-fold Cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic 4: Generalization and Regularization** Lectures 6-7\n",
    "* What is overfitting and underfitting?\n",
    "* What is the relationship of number of samples, model complexity and overfitting?\n",
    "* What is the bias-variance trade-off?\n",
    "* What can you do to prevent overfitting?\n",
    "* What is regularization and how is it used in the design of a predictive model?\n",
    "* Describe the difference between L1- and L2-norm regularizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic 5: Parameter Estimation** Lectures 8-10\n",
    "* What is MLE?\n",
    "* What is MAP?\n",
    "* What is the difference between MLE and MAP?\n",
    "* Discuss the effects of the prior belief in the estimation of MAP solutions.\n",
    "* What is a conjugate prior and how can we use it in online update of the model's parameters.\n",
    "* What is the Bayesian interpretation of the polynomial regression model with and without regularization term.\n",
    "* Derive the MLE and MAP solutions for the Bernoulli-Beta example in Lecture 9.\n",
    "* Review the final result for the MAP solution for the Gaussian-Gaussian example in Lecture 10.\n",
    "* Review the effects of prior and likelihood parameters in the online update code example presented in Lecture 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic 6: Classification** Lectures 11-12\n",
    "* How can we describe the two different types of classification?\n",
    "* Describe Naive Bayes Classifier.\n",
    "* Name at least one application of generative classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic 7: Data Likelihood Model and Clustering** Lectures 12-16\n",
    "* What are Mixture Models?\n",
    "* What are Gaussian Mixture Models (GMMs) used for?\n",
    "* How can we optimize a GMM likelihood model?\n",
    "* Describe the general steps of the Expectation-Maximization algorithm.\n",
    "* What type of optimization does EM use, and, does it converge to *global* solution?\n",
    "* Describe the pseudo-code for GMM optimized using the EM algorithm.\n",
    "* What is Clustering?\n",
    "* Describe the pseudo code for K-Means.\n",
    "* Why is GMM considered soft clustering?\n",
    "* Compare and discuss advantages/disadvantages of GMM vs k-Means.\n",
    "* Be able to analyze clustering results and identify which technique generated which result (Lecture 16 + Short Assignment 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic 8: Cluster Validity** Lecture 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Discuss challenges of validating an unsupervised model.\n",
    "* What type of index criteria are there when defining cluster validity metrics?\n",
    "* Describe the silhouette index.\n",
    "* Describe the rand index.\n",
    "* Be able to analyze a cluster validity result and determine how many clusters one should select."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post 4\n",
    "\n",
    "**Could you please go over online updating using conjugate prior relationship through an example?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw an example in Lecture 10, for the case Gaussian-Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the experiment where we flip *a* coin. We are interested in estimating the probability of flipping heads as new samples from this experiment arrive.\n",
    "\n",
    "Let heads=1 and tails=0, so our sample space is $S=\\{1,0\\}$. We can model the data likelihood as a Bernoulli distribution:\n",
    "\n",
    "$$P(x|\\mu) = \\mu^x(1-\\mu)^{1-x} = \\begin{cases}\\mu & \\text{if }x=1 \\\\ 1-\\mu & \\text{if } x=0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further assume that we have *prior knowledge* that the unknown parameter we are trying to estimate ($\\mu \\equiv$ probability of flipping heads) is a random variable and we will assume that it follows a Beta distribution:\n",
    "\n",
    "$$\\text{Beta}(\\mu|\\alpha,\\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1}$$\n",
    "\n",
    "where $\\Gamma(x) = (x-1)!$ and $\\alpha,\\beta>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply solve for the parameter $\\mu$ using the MAP approach:\n",
    "\n",
    "$$\\arg_{\\mu} \\max P(\\mu|X) = \\arg_{\\mu}\\max \\ln P(\\mu|X)$$\n",
    "\n",
    "Let $\\mathcal{L} = \\ln P(\\mu|X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the posterior probability as:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mu|X) &= \\frac{P(X|\\mu)P(\\mu)}{P(X)}\\\\\n",
    "&\\propto P(X|\\mu)P(\\mu)\\\\\n",
    "&\\propto \\left(\\prod_{n=1}^N \\mu^{x_n} (1-\\mu)^{1-x_n}\\right) \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} \\\\\n",
    "&= \\mu^m (1-\\mu)^l \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} \\\\\n",
    "&= \\mu^{m+\\alpha-1} (1-\\mu)^{l+\\beta-1}\n",
    "\\end{align}\n",
    "\n",
    "where $m$ the number of heads, $l$ the number of tails, and $N=m+l$ the total number of coin flips.\n",
    "\n",
    "* This defines a Conjugate Prior relationship as the prior and the posterior follow the same shape.\n",
    "\n",
    "* In an online environment (i.e. as new samples are coming), we can update the prior with the posterior distribution.By inspecting the posterior result, we see that:\n",
    "    * $\\alpha \\leftarrow \\alpha+m$ and $\\beta = \\beta + l$.\n",
    "    * We \"replace the prior with the posterior\" by updating the parameters of the prior distributions with those of the new posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then\n",
    "\n",
    "$$\\mathcal{L} = (m+\\alpha-1)\\ln(\\mu) + (l+\\beta-1)\\ln(1-\\mu)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now *optimize* our posterior probability:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial  \\ln \\left( P(\\mu|E) \\right)}{\\partial \\mu} &= 0\\\\\n",
    "\\frac{m+\\alpha-1}{\\mu} + \\frac{l+\\beta-1}{1-\\mu} &= 0\\\\\n",
    "\\mu &= \\frac{m+\\alpha-1}{m + l + \\alpha + \\beta -2}\n",
    "\\end{align}\n",
    "\n",
    "This is our estimation of the probability of heads using MAP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trueMU = 0.5 # 0.5 for a fair coin\n",
    "Nflips = 15\n",
    "a = 2\n",
    "b = 2\n",
    "\n",
    "xr = range(-1,3)\n",
    "x = np.linspace(-0.1,1.1,100)\n",
    "plt.plot(x, stats.beta(a,b).pdf(x))\n",
    "plt.xlabel('$\\mu$'); plt.ylabel('P($\\mu$)'); plt.title('Initial Prior')\n",
    "plt.show()\n",
    "Outcomes = []\n",
    "for i in range(Nflips):\n",
    "    Outcomes += [stats.bernoulli(trueMU).rvs(1)[0]]\n",
    "    estimate_mu = (np.sum(Outcomes)+a-1)/(len(Outcomes)+a+b-2)\n",
    "    \n",
    "    # Visualization:\n",
    "    fig=plt.figure(figsize=(15,5))\n",
    "    fig.add_subplot(1,2,1)\n",
    "    plt.stem(xr, stats.bernoulli(estimate_mu).pmf(xr))\n",
    "    plt.xlabel('$\\mu$'); plt.ylabel('P(X|$\\mu$)'); \n",
    "    plt.title('Data Likelihood, '+str(i+1)+' samples')\n",
    "    fig.add_subplot(1,2,2)\n",
    "    plt.plot(x, stats.beta(a,b).pdf(x))\n",
    "    plt.xlabel('$\\mu$'); plt.ylabel('P($\\mu$)'); plt.title('Posterior/Prior')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print estimate for mu\n",
    "    print('Data: ',Outcomes)\n",
    "    print('MAP estimate mu = ', estimate_mu)\n",
    "    \n",
    "    # Update Prior distribution\n",
    "    a = a+np.sum(Outcomes)\n",
    "    b = b+len(Outcomes)-np.sum(Outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post 5\n",
    "\n",
    "**If possible can we cover short assignment 3?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any specific questions?\n",
    "\n",
    "Short Assignment 3: https://ufl.instructure.com/courses/404363/assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post 6\n",
    "\n",
    "**In the supervised learning flowchart, there is a block called learning algorithm. Could you give an example of what we did in class that is a learning algorithm?  Is finding the best parameters in experimental design considered a learning algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, when we introduced classification using the Naive Bayes classifier, we define the model as a parametric representation of the data likelihood for each class. The objective is to maximize the data likelihood fitting. The learning algorithm is one that actually changes the parameters of the model. In this case, we can use either MLE or MAP as the learning algorithm to find the parameters of the data likelihood that maximize our objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post 7\n",
    "\n",
    "**Will the exam be more application or theory based? As in, will we be expected to perform large calculations/derivations or know the overall concept of ML thus far? I ask because a lot of these large functions can get very difficult to understand through derivations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exam will cover both conceptual questions, and derivation-like questions. \n",
    "\n",
    "To best prepare for the derivation-like questions, define the steps that are needed to take. For example, what are the steps to derive the best update equation for the $\\mu_k$, the mean of a Gaussian component in a GMM model?\n",
    "\n",
    "We will solve this using the EM algorithm:\n",
    "1. Write down the observed data likelihood\n",
    "2. Define the hidden latent variables\n",
    "3. Write down the complete data likelihood\n",
    "4. Take the log of the complete data likelihood\n",
    "5. Take the derivative of the log-likelihood with respect to $\\mu_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
