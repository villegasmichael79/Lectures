{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9 - MLE and MAP continued; Conjugate Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last class, we introduced the **Bayesian interpretation** of a supervised learning algorithm, and formulated two approaches for search for parameters of the model:\n",
    "\n",
    "1. **Maximum Likelihood Estimation (MLE)**\n",
    "\n",
    "2. **Maximum A Posteriori (MAP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our problem, the hypothesis are the *unknown* **(hyper-)parameters** $\\mathbf{w}$.\n",
    "\n",
    "* In Bayesian statistical inferencing, we are then trying to find the $\\mathbf{w}$'s that maximizing the posterior probability.\n",
    "* In classical statistical inferencing, on the other hand, we are only computing the probability of some hypothesis (the *null hypothesis*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<h2 align=\"center\"><span style=\"color:blue\">Maximum Likelihood Estimation (MLE)</span></h2>\n",
    "<center>(Frequentist approach)</center>\n",
    "\n",
    "$$\\arg_{\\mathbf{w}} \\max P(\\mathbf{x}|\\mathbf{w})$$\n",
    "\n",
    "In **Maximum Likelihood Estimation** (also referred to as **MLE** or **ML**) we want to *find the set of parameters* that **maximize** the data likelihood $P(\\mathbf{x}|\\mathbf{w})$. We want to find the *optimal* set of parameters under some assumed distribution such that the data is most likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<h2 align=\"center\"><span style=\"color:orange\">Maximum A Posteriori (MAP)</span></h2>\n",
    "<center>(Bayesian approach)</center>\n",
    "\n",
    "\\begin{align}\n",
    "& \\arg_{\\mathbf{w}} \\max P(\\mathbf{x}|\\mathbf{w})P(\\mathbf{w}) \\\\ \n",
    "& \\propto \\arg_{\\mathbf{w}} \\max P(\\mathbf{w}|\\mathbf{x})\n",
    "\\end{align}\n",
    "\n",
    "In **Maximum A Posteriori** (also referred as **MAP**) we want to *find the set of parameters* that **maximize** the posteriori probability $P(\\mathbf{w}|\\mathbf{x})$. We want to find the *optimal* set of parameters under some assumed distribution such that the parameters are most likely to have been drawn off of given some prior beliefs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Least Squares objective function in our regression problem, we showed that:\n",
    "\n",
    "* If we do not use a regularization term, then we are performing MLE. We are maximizing the data likelihood which takes the form of a Gaussian distribution.\n",
    "\n",
    "* If we use a regularization term, then we are performing MAP. We are maximizing the posterior distribution, which is equivalent at maximizing the data likelihood times the prior probability on the parameters.\n",
    "    * If we use *ridge regularizer*, both data likelihood and prior distribution take the form of a Gaussian.\n",
    "    * If we use *lasso regularizer*, data likelihood takes the form of a Gaussian distribution and the prior takes the form of a Laplacian distribution.\n",
    "    \n",
    "    \n",
    "Furthermore, this Bayesian interpretation allows us to think of a problem in a different way: if all I'm doing is maximizing a data likelihood or posterior, then I can design any distribution shape I want.\n",
    "\n",
    "* Often times, this lift is distributional form will be much more applicable to the data, as it fits better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **What is the distribution form of the error values for our model?**\n",
    "\n",
    "* **Why is it so common to assume Gaussian error distribution?** The official reason why people always assume a Gaussian error distribution goes back to something called the Central Limit Theorem. The Central Limit Theorem says that whenever a measurement is subject to a very large number of very small errors, the probability distribution for the total error is driven toward the Gaussian distribution. This is true regardless of the form of the original probability distributions of the individual errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "**Problem: Suppose I flip a coin 3 times and observe the event H-H-H. What is the probability of flipping Heads (H) on the next coin flip?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's consider heads=1 and tails=0, so our sample space is $S=\\{1,0\\}$. The probability of heads is equal to some *unknown* value $\\mu$, then:\n",
    "\n",
    "\\begin{align}\n",
    "& P(x=1 | \\mu) = \\mu \\\\\n",
    "& P(x=0|\\mu) = 1-\\mu\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the data likelihood as:\n",
    "\n",
    "$$P(x|\\mu) = \\mu^x(1-\\mu)^{1-x} = \\begin{cases}\\mu & \\text{if }x=1 \\\\ 1-\\mu & \\text{if } x=0 \\end{cases}$$\n",
    "\n",
    "* This is the **Bernoulli distribution**. The mean and variance of the Bernoulli distribution are: $E[x] = \\mu$ and $E[\\left(x- E[x]\\right)^2] = \\mu(1-\\mu)$.\n",
    "\n",
    "* So, for every outcome of the event $E$, we will model it using a Bernoulli distribution, and each outcome is pairwise **conditionally independent**. Therefore, we have the event $E$ contains i.i.d. outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Maximum Likelihood Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity of calculation, assume that the event contains outcomes: $E=x_1\\cap x_2\\cap \\dots\\cap x_N$, where $x_i=\\{0,1\\}$ (0 for Tails and 1 for Heads). Then, for an experiment with $N$ samples, we can write the **data likelihood** as:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "P(E|\\mu) &= P(x_1\\cap x_2\\cap \\dots\\cap x_N|\\mu) \\\\\n",
    "&= P(x_1|\\mu)P(x_2|\\mu)\\dots P(x_N|\\mu) \\\\\n",
    "&= \\prod_{n=1}^N P(x_n|\\mu) \\\\\n",
    "&= \\prod_{n=1}^N \\mu^{x_n} (1-\\mu)^{1-x_n}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, we are interested in finding the value of $\\mu$ given some data set $E$. \n",
    "\n",
    "We now optimize the data likelihood. What trick can we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$arg_\\mathbf{\\mu} \\max P(E|\\mu) = \\arg_\\mathbf{\\mu} \\max \\ln \\left( P(E|\\mu) \\right)$$\n",
    "\n",
    "because the $\\ln(\\bullet)$ is a monotonic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where \n",
    "$$\\ln(P(E|\\mu) = \\sum_{n=1}^N \\left(x_n \\ln(\\mu) + (1-x_n)\\ln(1-\\mu)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can take the derivative of this function wrt to $\\mu$ and equal it to zero:\n",
    "\n",
    "$$\\frac{\\partial \\ln(P(E|\\mu))}{\\partial \\mu} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "(1-\\mu)\\sum_{n=1}^N x_n - \\mu \\left(N - \\sum_{n=1}^N x_n\\right) &= 0 \\\\\n",
    "\\sum_{n=1}^N x_n - \\mu\\sum_{n=1}^N x_n - \\mu N + \\mu\\sum_{n=1}^N x_n &= 0 \\\\\n",
    "\\sum_{n=1}^N x_n - \\mu N &= 0 \\\\\n",
    "\\mu &= \\frac{1}{N} \\sum_{n=1}^N x_n\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the MLE estimation of the probability of seeing heads in the next coin flip is equal to **relative frequency** of outcome heads.\n",
    "\n",
    "* Suppose you flipped the coin only once, and saw Tails. The probability of flipping Heads according to MLE would be 0.\n",
    "\n",
    "* MLE is **purely data driven**! This is sufficient *when* we have lots and lots of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Maximum A Posteriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the MAP estimation of $\\mu$, we are instead optimizing the posterior probability:\n",
    "\n",
    "\\begin{align}\n",
    "&\\arg_{\\mu} \\max P(\\mu|E) \\\\\n",
    "=& \\arg_{\\mu} \\max \\frac{P(E|\\mu) P(\\mu)}{P(E)} \\\\\n",
    "\\propto & \\text{  } \\arg_{\\mu} \\max P(E|\\mu) P(\\mu), P(E)\\text{ is some constant value} \n",
    "\\end{align}\n",
    "\n",
    "We have defined the data likelihood $P(E|\\mu)$, we now need to choose a **prior distribution** $P(\\mu)$.\n",
    "\n",
    "* This prior distribution will *encode* any prior knowledge we have about the hidden sate of the problem, in this case, the type of coin that was used.\n",
    "\n",
    "Let's say our **prior distribution** is a Beta Distribution. A **Beta Distribution** takes the form:\n",
    "\n",
    "$$\\text{Beta}(x|\\alpha,\\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}$$\n",
    "\n",
    "where $\\Gamma(x) = (x-1)!$ and $\\alpha,\\beta>0$.\n",
    "\n",
    "The mean and variance of the Beta distribution are: $E[x] = \\frac{\\alpha}{\\alpha+\\beta}$ and $E[(x-E[x])^2] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's see what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "a = 2\n",
    "b = 20\n",
    "x = np.arange(0,1,0.0001)\n",
    "Beta = (math.gamma(a+b)/(math.gamma(a)*math.gamma(b)))*x**(a-1)*(1-x)**(b-1)\n",
    "\n",
    "plt.plot(x, Beta, label='Beta Distribution')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Probability of Heads, $\\mu$',fontsize=15)\n",
    "plt.ylabel('Prior Probability, p($\\mu$)',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Beat Distribution as out prior, we have:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mu|\\alpha,\\beta) &= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} \\\\\n",
    "&\\propto \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let:\n",
    "* $m$ the number of heads\n",
    "* $l$ the number of tails\n",
    "* $N=m+l$ the total number of coin flips \n",
    "\n",
    "We can write our **posterior probability** as:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mu|E) &= \\frac{P(E|\\mu)P(\\mu)}{P(E)}\\\\\n",
    "&\\propto P(E|\\mu)P(\\mu)\\\\\n",
    "&= \\left(\\prod_{n=1}^N \\mu^{x_n} (1-\\mu)^{1-x_n}\\right) \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} \\\\\n",
    "&= \\mu^m (1-\\mu)^l \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} \\\\\n",
    "&= \\mu^{m+\\alpha-1} (1-\\mu)^{l+\\beta-1}\n",
    "\\end{align}\n",
    "\n",
    "* The posterior probability has the same shape as the data likelihood. \n",
    "\n",
    "* This is a special case called **Conjugate Prior Relationship**, which happens when the posterior has the same form as the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now optimize our posterior probability, and we will apply the same trick:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$arg_\\mathbf{\\mu} \\max P(\\mu|E) = \\arg_\\mathbf{\\mu} \\max \\ln \\left( P(\\mu|E) \\right)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\ln \\left( P(\\mu|E) \\right) =  (m+\\alpha-1)\\ln(\\mu) + (l+\\beta-1)\\ln(1-\\mu)$$\n",
    "\n",
    "We can now *optimize* our posterior probability:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial  \\ln \\left( P(\\mu|E) \\right)}{\\partial \\mu} &= 0\\\\\n",
    "\\frac{m+\\alpha-1}{\\mu} + \\frac{l+\\beta-1}{1-\\mu} &= 0\\\\\n",
    "\\mu &= \\frac{m+\\alpha-1}{m + l + \\alpha + \\beta -2}\n",
    "\\end{align}\n",
    "\n",
    "This is our estimation of the probability of heads using MAP!\n",
    "\n",
    "* Our estimation for the probability of heads, $\\mu$, is going to depend on $\\alpha$ and $\\beta$ introduced by the prior distribution. We saw that they control the level of certainty as well as the center value.\n",
    "\n",
    "* With only a few samples, the prior will play a bigger role in the decision, but eventually the data takes over the prior.\n",
    "\n",
    "Let's run a simulation to compare MAP and MLE estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "MLE aka Frequentist Probability of Heads =  0.0\n",
      "MAP aka Bayesian Probability of Heads =  0.047619047619047616\n",
      "Press enter to flip the coin again...\n",
      "\n",
      "[0, 1]\n",
      "MLE aka Frequentist Probability of Heads =  0.5\n",
      "MAP aka Bayesian Probability of Heads =  0.09090909090909091\n",
      "Press enter to flip the coin again...\n",
      "\n",
      "[0, 1, 1]\n",
      "MLE aka Frequentist Probability of Heads =  0.6666666666666666\n",
      "MAP aka Bayesian Probability of Heads =  0.13043478260869565\n",
      "Press enter to flip the coin again...\n",
      "\n",
      "[0, 1, 1, 0]\n",
      "MLE aka Frequentist Probability of Heads =  0.5\n",
      "MAP aka Bayesian Probability of Heads =  0.125\n",
      "Press enter to flip the coin again...\n",
      "\n",
      "[0, 1, 1, 0, 0]\n",
      "MLE aka Frequentist Probability of Heads =  0.4\n",
      "MAP aka Bayesian Probability of Heads =  0.12\n",
      "Press enter to flip the coin again...\n",
      "\n",
      "[0, 1, 1, 0, 0, 1]\n",
      "MLE aka Frequentist Probability of Heads =  0.5\n",
      "MAP aka Bayesian Probability of Heads =  0.15384615384615385\n",
      "Press enter to flip the coin again...\n",
      "\n",
      "[0, 1, 1, 0, 0, 1, 0]\n",
      "MLE aka Frequentist Probability of Heads =  0.42857142857142855\n",
      "MAP aka Bayesian Probability of Heads =  0.14814814814814814\n",
      "Press enter to flip the coin again...\n",
      "\n",
      "[0, 1, 1, 0, 0, 1, 0, 0]\n",
      "MLE aka Frequentist Probability of Heads =  0.375\n",
      "MAP aka Bayesian Probability of Heads =  0.14285714285714285\n",
      "Press enter to flip the coin again...\n",
      "\n",
      "[0, 1, 1, 0, 0, 1, 0, 0, 1]\n",
      "MLE aka Frequentist Probability of Heads =  0.4444444444444444\n",
      "MAP aka Bayesian Probability of Heads =  0.1724137931034483\n",
      "Press enter to flip the coin again...\n",
      "\n",
      "[0, 1, 1, 0, 0, 1, 0, 0, 1, 0]\n",
      "MLE aka Frequentist Probability of Heads =  0.4\n",
      "MAP aka Bayesian Probability of Heads =  0.16666666666666666\n",
      "Press enter to flip the coin again...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trueMU = 0.5 # 0.5 for a fair coin\n",
    "Nflips = 10\n",
    "# prior parameters that we need to assume/choose\n",
    "a = 2\n",
    "b = 20\n",
    "\n",
    "Outcomes = []\n",
    "for i in range(Nflips):\n",
    "    Outcomes += [stats.bernoulli(trueMU).rvs(1)[0]] # each sample will be 1 if it's heads or 0 if it's tails\n",
    "    print(Outcomes)\n",
    "    print('MLE aka Frequentist Probability of Heads = ', np.sum(Outcomes)/len(Outcomes))\n",
    "    print('MAP aka Bayesian Probability of Heads = ', (np.sum(Outcomes)+a-1)/(len(Outcomes)+a+b-2))\n",
    "    input('Press enter to flip the coin again...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<h2 align=\"center\"><span style=\"color:blue\">Maximum Likelihood Estimation (MLE)</span></h2>\n",
    "<center>(Frequentist approach)</center>\n",
    "\n",
    "$$\\arg_{\\mathbf{w}} \\max P(\\mathbf{x}|\\mathbf{w})$$\n",
    "\n",
    "In **Maximum Likelihood Estimation** we *find the set of parameters* that **maximize** the data likelihood $P(\\mathbf{x}|\\mathbf{w})$. We find the *optimal* set of parameters under some assumed distribution such that the data is most likely.\n",
    "\n",
    "* MLE focuses on maximizing the data likelihood, which *usually* provides a pretty good estimate\n",
    "\n",
    "* A common trick to maximize the data likelihood is to maximize the log likelihood\n",
    "\n",
    "* MLE is purely data driven \n",
    "\n",
    "* MLE works best when we have lots and lots of data\n",
    "\n",
    "* MLE will likely overfit when we have small amounts of data or, at least, becomes unreliable\n",
    "\n",
    "* It estimates relative frequency for our model parameters. Therefore it needs incredibly large amounts of data (infinite!) to estimate the true likelihood parameters\n",
    "    * This is a problem when we want to make inferences and/or predictions outside the range of what the training data has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<h2 align=\"center\"><span style=\"color:orange\">Maximum A Posteriori (MAP)</span></h2>\n",
    "<center>(Bayesian approach)</center>\n",
    "\n",
    "\\begin{align}\n",
    "& \\arg_{\\mathbf{w}} \\max P(\\mathbf{x}|\\mathbf{w})P(\\mathbf{w}) \\\\ \n",
    "& \\propto \\arg_{\\mathbf{w}} \\max P(\\mathbf{w}|\\mathbf{x})\n",
    "\\end{align}\n",
    "\n",
    "In **Maximum A Posteriori** we *find the set of parameters* that **maximize** the the posterior probability $P(\\mathbf{w}|\\mathbf{x})$. We find the *optimal* set of parameters under some assumed distribution such that the parameters are most likely to have been drawn off of.\n",
    "\n",
    "* MAP focuses on maximizing the posterior probability - data  likelihood with a prior\n",
    "\n",
    "* A common trick to maximize the posterior probability is to maximize the log likelihood\n",
    "\n",
    "* MAP is data driven \n",
    "\n",
    "* MAP is mostly driven by the prior beliefs\n",
    "\n",
    "* MAP works great with small amounts of data *if* our prior was chosen well\n",
    "\n",
    "* We need to assume and select a distribution for our prior beliefs\n",
    "    * A wrong choice of prior distribution can impact negatively our model estimation\n",
    "    \n",
    "* When we have lots and lots of data, the data likelihood will take over and the posterior will depend less and less on the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two distributions have a **conjugate prior** relationship when the form of the posterior is the same as the form of the prior.\n",
    "\n",
    "    * For example, consider the data likelihood $P(X|\\mu) \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and the prior distribution $P(\\mu) \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$. The posterior probability will also be Gaussian distributed\n",
    "    \n",
    "$$P(\\mu|X) \\sim \\mathcal{N}\\left(\\frac{\\sum_{i=1}^N x_i\\sigma_0^2 + \\mu_0\\sigma^2}{N\\sigma_0^2+\\sigma^2},\\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right)^{-1}\\right)$$\n",
    "\n",
    "<!-- Proof: \n",
    "\n",
    "\\begin{align}\n",
    "P(\\mu|X) &\\propto P(X|\\mu)P(\\mu) \\\\\n",
    "& = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2}\\frac{(x_i-\\mu)^2}{\\sigma^2}\\right)\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2}\\frac{(\\mu-\\mu_0)^2}{\\sigma_0^2}\\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(\\sum_{i=1}^N \\left(-\\frac{1}{2}\\frac{(x_i-\\mu)^2}{\\sigma^2}\\right) -\\frac{1}{2}\\frac{(\\mu-\\mu_0)^2}{\\sigma_0^2} \\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2}\\left(\\sum_{i=1}^N \\frac{(x_i-\\mu)^2}{\\sigma^2} +\\frac{(\\mu-\\mu_0)^2}{\\sigma_0^2} \\right) \\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{\\sum_{i=1}^N x_i^2-2\\sum_{i=1}^N x_i\\mu +\\mu^2N)}{\\sigma^2} +\\frac{\\mu^2-2\\mu\\mu_0 +\\mu_0^2}{\\sigma_0^2} \\right) \\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2} \\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right) - 2\\mu \\left( \\frac{\\sum_{i=1}^N x_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right) + \\frac{\\sum_{i=1}^N x_i^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\sigma_0^2}\\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2} \\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right) \\left(\\mu^2 - 2\\mu\\left(\\frac{\\sum_{i=1}^N x_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right)\\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right)^{-1} \\right) \\right) \\exp\\left(\\frac{\\sum_{i=1}^N x_i}{\\sigma^2} + \\frac{\\mu_0^2}{\\sigma_0^2}\\right)\n",
    "\\end{align} -->\n",
    "\n",
    "* There are many conjugate prior relationships, e.g., Bernoulli-Beta, Gausian-Gaussian, Gaussian-Inverse Wishart, Multinomial-Dirichlet.\n",
    "\n",
    "* Conjugate prior relationships play an important role for online **updating** our prior distribution.\n",
    "\n",
    "* In an online model estimation scenario, where we the posterior has the same form as the prior, we can use the posterior as our new prior. This new prior is now data informative and will update it's parameters based on (1) our initial choice, and (2) the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Online Updating of the Prior using Conjugate Priors (Gaussian-Gaussian)\n",
    "\n",
    "Let's consider the example presented in the Bishop textbook (Figure 3.7 in page 155).\n",
    "\n",
    "Consider a single input variable $x$, a single target variable $t$ and a linear model of the form $y(x,w) = w_0 + w_1x$.\n",
    "Because this has just two parameters coefficients, $w=[w_0, w_1]^T$, we can plot the prior and posterior distributions directly in parameter space (2-dimensional parameter space).\n",
    "\n",
    "Let's generate some synthetic data from the function $f(x, a) = w_0 + w_1x$ with parameter values $w_0 = −0.3$ and $w_1 = 0.5$ by first choosing values of $x_n$ from the uniform distribution $U(x_n|−1, 1)$, then evaluating $f(x_n, \\mathbf{w})$, and finally adding Gaussian noise with standard deviation of $\\sigma = 0.2$ to obtain the target values $t_n$.\n",
    "\n",
    "$$t_n = f(x_n, \\mathbf{w}) + \\epsilon = -0.3 + 0.5 x_n + \\epsilon\\text{, where }\\epsilon\\sim N(0,0.2^2) $$\n",
    "\n",
    "* Our **goal** is to recover the values of $w_0$ and $w_1$ from such data, and we will explore the dependence on the size of the data set.\n",
    "\n",
    "For some data, $\\{x_n,t_n\\}_{n=1}^N$, we can pose this problem in terms of **Regularized Least Squares**:\n",
    "\n",
    "\\begin{align}\n",
    "E(\\mathbf{w}) &= \\frac{1}{N} \\sum_{n=1}^N \\left(t_n - y_n\\right)^2 + \\lambda \\sum_{i=0}^1 w_i^2 \\\\\n",
    "&= \\frac{1}{N} \\sum_{n=1}^N \\left(t_n - y_n\\right)^2 + \\lambda \\left(w_0^2 + w_1^2\\right)\\\\\n",
    "& \\Rightarrow \\arg_{\\mathbf{w}}\\min J(\\mathbf{w})\n",
    "\\end{align}\n",
    "\n",
    "Using **MAP**, we can rewrite our objective using the **Bayesian interpretation**:\n",
    "\n",
    "\\begin{align}\n",
    "\\arg_{\\mathbf{w}} \\max P(\\mathbf{e}|\\mathbf{w})P(\\mathbf{w})\n",
    "\\end{align}\n",
    "\n",
    "Let's consider the data likelihood, $P(\\mathbf{e}|\\mathbf{w})$, to be a Gaussian distribution with mean $\\mu = 0$ and variance $\\sigma^2 = 0.2^2$. And let's also consider the prior distribution, $P(\\mathbf{w})$, to be a Gaussian distribution with mean $\\mu_0$ and variance $\\sigma_0^2$. Then we can rewrite our optimization as:\n",
    "\n",
    "\\begin{align}\n",
    "\\arg_{\\mathbf{w}} \\max N(\\mathbf{e}|0,0.2^2)N(\\mathbf{w}|\\mu_0,\\sigma_0)\n",
    "\\end{align}\n",
    "\n",
    "Note that we **do not known** the parameters of the prior distribution. The parameters of the prior distribution will have to be chosen by the user (us). And they will essentially *encode* any behavior or a priori knowledge we may have about the weights.\n",
    "\n",
    "Both our data likelihood and prior distributions are in a 2-dimensional space (this is because our *model order* is $M=2$ -- we have 2 parameters!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Prior Relationship\n",
    "\n",
    "For a D-dimensional Gaussian data likelihood with mean $\\mu$ and covariance $\\beta\\mathbf{I}$ and a prior distribution with mean $\\mu_0$ and covariance $\\Sigma_0$\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mathbf{e}|\\mathbf{w}) &\\sim N(\\mathbf{\\mu}, \\beta\\mathbf{I}) \\\\\n",
    "P(\\mathbf{w}) &\\sim N(\\mathbf{\\mu}_0,\\Sigma_0)\n",
    "\\end{align}\n",
    "\n",
    "The posterior distribution\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mathbf{w}|\\mathbf{e}) &\\sim N\\left(\\mathbf{\\mu}_N, \\Sigma_N\\right) \\\\\n",
    "\\mathbf{\\mu}_N &= \\Sigma_N \\left(\\Sigma_0^{-1}\\mathbf{\\mu}_0+\\beta\\mathbf{\\Phi}^T\\mathbf{t}\\right)\\\\\n",
    "\\Sigma_N^{-1} &= \\Sigma_0^{-1} + \\beta \\mathbf{\\Phi}^T\\mathbf{\\Phi}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{\\Phi}$ is the matrix of features extracted from the data samples $\\{x_i\\}_{i=1}^N$. If we consider polynomial features, then $\\mathbf{\\phi}(\\mathbf{x}_k) = \\left[x_k^0, x_k^1, \\cdots, x_k^M \\right]$.\n",
    "\n",
    "* What happens with different values of $\\beta$ and $\\Sigma_0$?\n",
    "\n",
    "To simplify, let's assume $\\Sigma_0 = \\alpha^{-1}\\mathbf{I}$ and $\\mathbf{\\mu}_0 = [0,0]$, thus \n",
    "\n",
    "$$\\mu_N = \\beta \\Sigma_N\\mathbf{\\Phi}^T\\mathbf{t}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\Sigma_N^{-1} = (\\alpha^{-1}\\mathbf{I})^{-1} + \\beta\\mathbf{\\Phi}^T \\mathbf{\\Phi} = \\alpha\\mathbf{I} + \\beta \\mathbf{\\Phi}^T \\mathbf{\\Phi} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Coming Back) Example of Online Updating of the Prior using Conjugate Priors (Gaussian-Gaussian)\n",
    "\n",
    "In the example introduced above, our data likelihood and prior distributions are in a 2-dimensional space ruled by the model parameters $\\mathbf{w}=[w_0,w_1]^T$, one axis is $w_0$ and another is $w_1$.\n",
    "\n",
    "We are going to generate data from $t = -0.3 + 0.5x + \\epsilon$ where $\\epsilon$ is drawn from a zero-mean Gaussin distribution.\n",
    "\n",
    "* **The goal is to estimate the values $w_0=-0.3$ and $w_1=0.5$**\n",
    "\n",
    "We want to implement this scenario for a case that we are getting more data every minute. As we get more and more data, we want to update our prior distribution using our posterior distribution (informative prior). This is only possible because because Gaussian-Gaussian have a conjugate prior relationship. That is, the posterior distribution is also a Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_prior_func():   \n",
    "    fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    # true weights\n",
    "    a = -0.3 # w0\n",
    "    b = 0.5  # w1\n",
    "    \n",
    "    # set up input space range\n",
    "    rangeX = [-2, 2]\n",
    "    step = 0.025\n",
    "    X = np.mgrid[rangeX[0]:rangeX[1]:step]\n",
    "    \n",
    "    # parameters to choose\n",
    "    alpha = 20 # 1/variance = precision of the prior\n",
    "    beta = 2   # 1/standard deviation = square of precision of additive Gaussian noise\n",
    "    S0 = (1/alpha)*np.eye(2) # prior covariance matrix\n",
    "    draw_num = (0,1,10,20,50,100) # number of points to draw\n",
    "\n",
    "    #initialize prior/posterior and sample data\n",
    "    sigma = S0\n",
    "    mean = [0,0]\n",
    "    draws = np.random.uniform(rangeX[0],rangeX[1],size=draw_num[-1])\n",
    "    T = a + b*draws + np.random.normal(loc=0, scale=math.sqrt(1/beta))\n",
    "\n",
    "    for i in range(len(draw_num)):\n",
    "        if draw_num[i]>0: #skip first image\n",
    "            #Show data likelihood\n",
    "            Phi = np.vstack((np.ones(draws[0:draw_num[i]].shape), draws[0:draw_num[i]])) #Polynomial features\n",
    "            t = T[0:draw_num[i]]\n",
    "            sigma = np.linalg.inv(S0 + beta*Phi@Phi.T)\n",
    "            mean = beta*sigma@Phi@t\n",
    "\n",
    "            w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "            p = multivariate_normal(t[draw_num[i]-1], 1/beta)\n",
    "            out = np.empty(w0.shape)\n",
    "            for j in range(len(w0)):\n",
    "                out[j] = p.pdf(w0[j]+w1[j]*draws[draw_num[i]-1])\n",
    "\n",
    "            ax = fig.add_subplot(*[len(draw_num),3,(i)*3+1])\n",
    "            ax.pcolor(w0, w1, out)\n",
    "            ax.scatter(a,b, c='c')\n",
    "            myTitle = 'data likelihood'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        #Show prior/posterior\n",
    "        w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "        pos = np.empty(w1.shape + (2,))\n",
    "        pos[:, :, 0] = w0; pos[:, :, 1] = w1\n",
    "        p = multivariate_normal(mean, sigma)\n",
    "\n",
    "        ax = fig.add_subplot(*[len(draw_num),3,(i)*3+2])\n",
    "        ax.pcolor(w0, w1, p.pdf(pos))\n",
    "        ax.scatter(a,b, c='c')\n",
    "        myTitle = 'Prior/Posterior'\n",
    "        ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        #Show data space\n",
    "        for j in range(6):\n",
    "            w0, w1 = np.random.multivariate_normal(mean, sigma)\n",
    "            t = w0 + w1*X\n",
    "            ax = fig.add_subplot(*[len(draw_num),3,(i)*3+3])\n",
    "            ax.plot(X,t)\n",
    "            if draw_num[i] > 0:\n",
    "                ax.scatter(Phi[1,:], T[0:draw_num[i]])\n",
    "            myTitle = 'data space'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "likelihood_prior_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
