{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 21 - Discriminant Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continued Discussion: $k$-Nearest Neighbors Classifier\n",
    "\n",
    "Nearest neighbors methods compare a test point to the $k$ nearest training data points and then estimate an output value based on the desired/true output values of the $k$ nearest training points.\n",
    "\n",
    "* Essentially, there is no \"training\" other than storing the training data points and their desired outputs\n",
    "\n",
    "* In test, you need to: \n",
    "    1. Determine which $k$ neighbors in the training data are closest to the test point; and,\n",
    "    2. Determine the output value for the test point.\n",
    "    \n",
    "In order to find the $k$ *nearest-neighbors* in the training data, you need to define a **similarity measure** or a **dissimilarity measure**. The most common dissimilarity measure is Euclidean distrance:\n",
    "\n",
    "* Euclidean distance: $d_E(\\mathbf{x}_1, \\mathbf{x}_2) = \\sqrt{(\\mathbf{x}_1 - \\mathbf{x}_2)^T(\\mathbf{x}_1 - \\mathbf{x}_2)}$\n",
    "\n",
    "* City-block distance: $d_{CB}(\\mathbf{x}_1,\\mathbf{x}_2) = \\sum_{i=1}^n |\\mathbf{x}_{1i} - \\mathbf{x}_{2i}|$\n",
    "\n",
    "* Mahalanobis distance: $d_M(\\mathbf{x}_1, \\mathbf{x}_2) = \\sqrt{(\\mathbf{x}_1 - \\mathbf{x}_2)^T\\Sigma^{-1}(\\mathbf{x}_1 - \\mathbf{x}_2)}$\n",
    "\n",
    "* Cosine angle similarity: $\\cos(\\theta) = \\frac{\\mathbf{x}_1^T \\mathbf{x}_2}{\\Vert\\mathbf{x}_1\\Vert_2^2 \\Vert\\mathbf{x}_2\\Vert_2^2}$\n",
    "\n",
    "* and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are doing classification, once you find the $k$ nearest neighbors to your test point in the training data, then you can determine the class label of your test point using (most commonly) **majority vote**.\n",
    "\n",
    "* If there are ties, they can be broken randomly or using schemes like applying the label to the closest data point in the neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-colorblind')\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification, make_blobs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\n",
    "\n",
    "# figure parameters\n",
    "h = .02  # step size in the mesh\n",
    "figure = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# set up classifiers\n",
    "n_neighbors = 3\n",
    "classifiers = [KNeighborsClassifier(n_neighbors, weights='uniform'), \\\n",
    "               KNeighborsClassifier(n_neighbors, weights='distance')]\n",
    "names = [\"k-NN Uniform\", \"k-NN Weighted\"]\n",
    "\n",
    "# Put together Data Sets\n",
    "n_samples = 300\n",
    "X, y = make_classification(n_samples, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.3, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 2, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k',label='Training points')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], marker='+', c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k',label='Test points')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.legend()\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 2, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_predict = clf.predict(X_test)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "#             Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], marker='+', c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        \n",
    "        i += 1\n",
    "    # ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_predict)\n",
    "    C = confusion_matrix(y_test, y_predict)\n",
    "    fig = plt.subplot(len(datasets), len(classifiers) + 2, i)\n",
    "    sns.heatmap(C,annot=True)\n",
    "    \n",
    "    \n",
    "#     fig.plot(fpr,tpr)\n",
    "#     plt.xlabel('FPR')\n",
    "#     plt.ylabel('TPR')\n",
    "#     plt.title(auc(fpr, tpr))\n",
    "    i += 1\n",
    "        \n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Functions\n",
    "\n",
    "So far we designed classifiers based on probability density or probability functions. In some cases, we saw that the resulting classifiers were equivalent to a set of linear discriminant functions. \n",
    "\n",
    "We will now focus on the design of linear classifiers, irrespective of the underlying distributions describing the training data. \n",
    "* The major advantage of linear classifiers is their simplicity and computational attractiveness.\n",
    "\n",
    "* We will develop techniques for the computation of the corresponding linear functions. In the sequel we will focus on a more general problem, in which a linear classifier cannot classify correctly all feature vectors, yet we will seek ways to design an optimal linear classifier by adopting an appropriate optimality criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear discriminant functions are typically presented for a 2-class problem due to its geometry interpretation.\n",
    "* Linear discriminant functions are generalized to $K>2$ classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn 3 methods to optimize the parameters of a linear discriminant function (classifier):\n",
    "\n",
    "1. Least Squares Classification\n",
    "2. Fisher's Linear Discriminant\n",
    "3. The Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Decision Boundary\n",
    "\n",
    "Suppose we have a 2-class problem and we want to find a *linear boundary* that separates the two classes, such that, above the decision boundary all points belong to one class and below the decision boundary all points belong to the other class.\n",
    "\n",
    "* To do this, we would want the mean of the two classes to be as far a part as possible, and the variance of each class to be as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can design a cost function as the ratio between the squared difference of the means over the sum of the variances. In order to find a linear decision boundary, we want to maximize this cost function.\n",
    "\n",
    "$$\\frac{(m_2 - m_1)^2}{s_1^2 + s_2^2}$$\n",
    "\n",
    "This cost function is a function of the parameters $\\mathbf{w}$ that characterize the line or hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The line of hyperplane perpendicular to the decision boundary will point in the **direction of projection** that preserves class separability.\n",
    "\n",
    "* We will end up with far and compact clusters which are easy to linearly separate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with linear discriminant functions:\n",
    "$$y(\\mathbf{\\overrightarrow{x}}) = \\mathbf{\\overrightarrow{w}}^T\\mathbf{x} + w_0$$\n",
    "\n",
    "*Looks pretty familiar, right?* If you are on one side of the line, then you are in class 1.  If you are on the other side of the line, then you are in class 2.  So, the decision boundary is $y(\\mathbf{\\overrightarrow{x}}) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The distance of a point to the decision boundary is: $\\frac{y(\\mathbf{\\overrightarrow{x}})}{\\left\\| \\mathbf{\\overrightarrow{w}} \\right\\|}$ \n",
    "    * See Figure 4.1 from textbook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('figures/Figure4.1.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a **least squares** error function to solve for $\\mathbf{\\overrightarrow{w}}$ and $w_0$ as we did in regression.  But, there are some issues. *Can you think of any?*\n",
    "\n",
    "* In regression, the prediction label will be a continuous number between $[-1,1]$. So the predicted class label will be for example: -0.8, 0.4 or 0.01. To simplify, we can say, if the predicted class $y \\geq 0$ than is class 1 otherwise is class 0.\n",
    "* The problem that comes about is that, if we look at the distribution of our errors, in our estimation $\\epsilon = t-y$ is not Gaussian.    \n",
    "* The errors samples are assumed independent, with a mean and a variance independent from each other.\n",
    "* If we use regression, what we going end up with is an error distribution where the variance is dependent on the mean. This becomes a signal-dependent problem therefore regression is not a good approach to classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher's Linear Discriminant\n",
    "\n",
    "A very popular type of a linear discriminant is the **Fisher's Linear Discriminant**.\n",
    "\n",
    "* Given two classes, we can compute the mean of each class:\n",
    "$$\\mathbf{\\overrightarrow{m}}_1 = \\frac{1}{{N}_1}\\sum_{n\\in C_1} \\mathbf{\\overrightarrow{x}_n}$$\n",
    "\n",
    "$$\\mathbf{\\overrightarrow{m}_2} = \\frac{1}{N_2}\\sum_{n\\in C_2} \\mathbf{\\overrightarrow{x}_n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can maximize the separation of the means:\n",
    "\n",
    "$$m_2 - m_1 = \\mathbf{\\overrightarrow{w}}^T(\\mathbf{\\overrightarrow{m}}_2 - \\mathbf{\\overrightarrow{m}}_1)$$\n",
    "\n",
    "* $\\mathbf{\\overrightarrow{w}}^T\\mathbf{\\overrightarrow{x}}$ takes a $D$ dimensional data point and projects it down to 1-D with a weight sum of the original features. We want to find a weighting that maximizes the separation of the class means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Not only do we want well separated means for each class, but we also want each class to be *compact* to minimize overlap between the classes. \n",
    "\n",
    "* Consider the *within class variance:*\n",
    "\n",
    "\\begin{align}\n",
    "s_k^2 = \\sum_{n \\in C_k} (y_n - m_k)^2 &= \\sum_{n \\in C_k} (\\mathbf{\\overrightarrow{w}}^T\\mathbf{\\overrightarrow{x}}_n - m_k)^2 \\\\\n",
    "&= \\mathbf{\\overrightarrow{w}}^T \\sum_{n \\in C_k} (\\mathbf{\\overrightarrow{x}}_n - \\mathbf{\\overrightarrow{m}_k}) (\\mathbf{\\overrightarrow{x}}_n - \\mathbf{\\overrightarrow{m}_k})^T \\mathbf{\\overrightarrow{w}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, we want to minimize within class variance and maximize between class separability. How about the following objective function:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "J(\\mathbf{w}) &=& \\frac{(m_2 - m_1)^2}{s_1^2 + s_2^2}\\\\\n",
    "      &=& \\frac{\\mathbf{\\overrightarrow{w}}^T(\\mathbf{\\overrightarrow{m}}_2 - \\mathbf{\\overrightarrow{m}}_1)(\\mathbf{\\overrightarrow{m}}_2 - \\mathbf{\\overrightarrow{m}}_1)^T\\mathbf{\\overrightarrow{w}}}{\\sum_{n \\in C_1} (\\mathbf{\\overrightarrow{w}}^T\\mathbf{\\overrightarrow{x}}_n - m_1)^2 + \\sum_{n \\in C_2} (\\mathbf{\\overrightarrow{w}}^T\\mathbf{\\overrightarrow{x}}_n - m_2)^2}\\\\\n",
    "\t  &=& \\frac{\\mathbf{\\overrightarrow{w}}^T(\\mathbf{\\overrightarrow{m}}_2 - \\mathbf{\\overrightarrow{m}}_1)(\\mathbf{\\overrightarrow{m}}_2 - \\mathbf{\\overrightarrow{m}}_1)^T\\mathbf{\\overrightarrow{w}}}{\\mathbf{\\overrightarrow{w}}^T\\left(\\sum_{n \\in C_1} (\\mathbf{\\overrightarrow{x}}_n - \\mathbf{\\overrightarrow{m}}_1)(\\mathbf{\\overrightarrow{x}}_n - \\mathbf{\\overrightarrow{m}}_1)^T + \\sum_{n \\in C_2} (\\mathbf{\\overrightarrow{x}}_n - \\mathbf{\\overrightarrow{m}}_2)(\\mathbf{\\overrightarrow{x}}_n - \\mathbf{\\overrightarrow{m}}_2)^T\\right)\\mathbf{\\overrightarrow{w}}}\\\\\n",
    "\t  &=& \\frac{\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_B\\mathbf{\\overrightarrow{w}}}{\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_W\\mathbf{\\overrightarrow{w}}}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "$$S_B = (\\mathbf{\\overrightarrow{m}}_2 - \\mathbf{\\overrightarrow{m}}_1)(\\mathbf{\\overrightarrow{m}}_2 - \\mathbf{\\overrightarrow{m}}_1)^T$$\n",
    "\n",
    "and \n",
    "\n",
    "$$S_W= \\sum_{n \\in C_1} (\\mathbf{\\overrightarrow{x}}_n - \\mathbf{\\overrightarrow{m}}_1)(\\mathbf{\\overrightarrow{x}}_n - \\mathbf{\\overrightarrow{m}}_1)^T + \\sum_{n \\in C_2} (\\mathbf{\\overrightarrow{x}}_n - \\mathbf{\\overrightarrow{m}}_2)(\\mathbf{\\overrightarrow{x}}_n - \\mathbf{\\overrightarrow{m}}_2)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ok, so let's optimize:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial J(\\mathbf{\\overrightarrow{w}})}{\\partial \\mathbf{\\overrightarrow{w}}} &=& \\frac{2(\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_W\\mathbf{\\overrightarrow{w}})\\mathbf{S}_B\\mathbf{\\overrightarrow{w}} - 2(\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_B\\mathbf{\\overrightarrow{w}})\\mathbf{S}_W\\mathbf{\\overrightarrow{w}}}{(\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_W\\mathbf{\\overrightarrow{w}})^2} = 0 \\\\\n",
    "0 &=& \\ \\frac{\\mathbf{S}_B\\mathbf{\\overrightarrow{w}}}{(\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_W\\mathbf{\\overrightarrow{w}})} -\\frac{(\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_B\\mathbf{\\overrightarrow{w}})\\mathbf{S}_W\\mathbf{\\overrightarrow{w}}}{(\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_W\\mathbf{\\overrightarrow{w}})^2} \\\\\n",
    " (\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_W\\mathbf{\\overrightarrow{w}})\\mathbf{S}_B\\mathbf{\\overrightarrow{w}} &=& (\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_B\\mathbf{\\overrightarrow{w}})\\mathbf{S}_W\\mathbf{\\overrightarrow{w}} \\\\\n",
    " \\mathbf{S}_B \\mathbf{\\overrightarrow{w}} &=& \\frac{\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_B\\mathbf{\\overrightarrow{w}}}{\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_W\\mathbf{\\overrightarrow{w}}} \\mathbf{S}_W\\mathbf{\\overrightarrow{w}}\\\\\n",
    " \\mathbf{S}_W^{-1}\\mathbf{S}_B\\mathbf{\\overrightarrow{w}} &=& \\lambda\\mathbf{\\overrightarrow{w}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "where the scalar $\\lambda = \\frac{\\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_B\\mathbf{\\overrightarrow{w}}} { \\mathbf{\\overrightarrow{w}}^T\\mathbf{S}_W\\mathbf{\\overrightarrow{w}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Does this look familiar?**\n",
    "\n",
    "This is the generalized eigenvalue problem!\n",
    "\n",
    "* So the direction of projection correspond to the eigenvectors of $\\mathbf{S}_W^{-1}\\mathbf{S}_B$ with the largest eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is easy when $S_w^{-1} = (\\Sigma_1 + \\Sigma_2)^{-1}$ exists.\n",
    "\n",
    "In this case, if we use the definition of $S_B = (\\mathbf{\\overrightarrow{m}}_2 - \\mathbf{\\overrightarrow{m}}_1)(\\mathbf{\\overrightarrow{m}}_2 - \\mathbf{\\overrightarrow{m}}_1)^T$:\n",
    "\n",
    "\\begin{align}\n",
    "S_W^{-1}S_B\\mathbf{\\overrightarrow{w}} &= \\lambda\\mathbf{\\overrightarrow{w}}\\\\\n",
    "S_W^{-1}(\\mathbf{\\overrightarrow{m}}_2-\\mathbf{\\overrightarrow{m}}_1)(\\mathbf{\\overrightarrow{m}}_2-\\mathbf{\\overrightarrow{m}}_1)^T\\mathbf{\\overrightarrow{w}} &= \\lambda\\mathbf{\\overrightarrow{w}}\n",
    "\\end{align}\n",
    "\n",
    "Noting that $\\alpha = (\\mathbf{\\overrightarrow{m}}_2-\\mathbf{\\overrightarrow{m}}_1)^T\\mathbf{\\overrightarrow{w}}$ is a constant, this can be written as:\n",
    "\n",
    "$$S_W^{-1}(\\mathbf{\\overrightarrow{m}}_2-\\mathbf{\\overrightarrow{m}}_1) = \\frac{\\lambda}{\\alpha}\\mathbf{\\overrightarrow{w}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since we don't care about the magnitude of $\\mathbf{\\overrightarrow{w}}$:\n",
    "\n",
    "$$\\mathbf{\\overrightarrow{w}}^* = S_W^{-1}(\\mathbf{\\overrightarrow{m}}_2-\\mathbf{\\overrightarrow{m}}_1) = (\\Sigma_1 + \\Sigma_2)^{-1}(\\mathbf{\\overrightarrow{m}}_2-\\mathbf{\\overrightarrow{m}}_1)$$\n",
    "\n",
    "Make sure $\\mathbf{\\overrightarrow{w}}^*$ is a unit vector by taking: $\\mathbf{\\overrightarrow{w}}^* \\leftarrow \\frac{\\mathbf{\\overrightarrow{w}}^*}{\\Vert\\mathbf{\\overrightarrow{w}}^*\\Vert}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that if the within-class covariance, $S_W$, is isotropic, so that $S_W$ is proportional to the unit matrix, we find that $\\mathbf{\\overrightarrow{w}}$ is proportional to the difference of the class means.\n",
    "\n",
    "* This result is known as *Fisher's linear discriminant*, although strictly it is not a discriminant but rather a specific choice of direction for projection of the data down to one dimension. However, the projected data can subsequently be used to construct a discriminant, by choosing a threshold $y_0$ so that we classify a new point as belonging to $C_1$ if $y(x) \\geq y_0$ and classify it as belonging to $C_2$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, note that:\n",
    "* For a classification problem with Gaussian classes of equal covariance $\\Sigma_i=\\Sigma$, the boundary is the plane of normal:\n",
    "$$\\mathbf{\\overrightarrow{w}} = \\Sigma^{-1}(\\mathbf{\\overrightarrow{m}}_i-\\mathbf{\\overrightarrow{m}}_j)$$\n",
    "\n",
    "* If $\\Sigma_2=\\Sigma_1$, this is also the LDA solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives two different **interpretations** of LDA:\n",
    "\n",
    "* It is optimal *if and only if* the classes are Gaussian and have equal covariance.\n",
    "\n",
    "* A classifier on the LDA features, is equivalent to the boundary after the approximation of the data by two Gaussians with equal covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final discriminant decision boundary is $\\mathbf{\\overrightarrow{y}} = \\mathbf{\\overrightarrow{w}}^*\\mathbf{\\overrightarrow{x}} + w_0$\n",
    "\n",
    "The *bias* term $w_0$ can be defined as:\n",
    "\n",
    "$$w_0 = \\left(\\frac{1}{N_1}\\sum_{n\\in C_1}\\overrightarrow{x}_n + \\frac{1}{N_2}\\sum_{n\\in C_2}\\overrightarrow{x}_n \\right)\\mathbf{\\overrightarrow{w}}^*$$\n",
    "\n",
    "* An extension to multi-class problems has a similar derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations** of LDA:\n",
    "\n",
    "1. LDA produces at most $C-1$ feature projections, where $C$ is the number of classes.\n",
    "\n",
    "2. If the classification error estimates establish that more features are needed, some other method must be employed to provide those additional features.\n",
    "\n",
    "3. LDA is a parametric method (it assumes unimodal Gaussian likelihoods).\n",
    "\n",
    "4. If the distributions are significantly non-Gaussian, the LDA projections may not preserve complex structure in the data needed for classification.\n",
    "\n",
    "5. LDA will also fail if discriminatory information is not in the mean but in the variance of the data.\n",
    "\n",
    "A popular variant of LDA are the **Multi-Layer Perceptrons** (or MLPs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisherDiscriminant(data,t):\n",
    "    data1 = data[t==0,:]\n",
    "    data2 = data[t==1,:]\n",
    "    mean1 = np.atleast_2d(np.mean(data1,0))\n",
    "    mean2 = np.atleast_2d(np.mean(data2,0))\n",
    "    Sw1 = np.dstack([(data1[i,:]-mean1).T@(data1[i,:]-mean1) for i in range(data1.shape[0])])\n",
    "    Sw2 = np.dstack([(data2[i,:]-mean2).T@(data2[i,:]-mean2) for i in range(data2.shape[0])])\n",
    "    Sw = np.sum(Sw1,2) + np.sum(Sw2,2)\n",
    "    w = np.linalg.inv(Sw)@(mean2 - mean1).T\n",
    "    w = w/np.linalg.norm(w)\n",
    "    data_t = data@w\n",
    "    return w, data_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminant(data, labels, v):\n",
    "    v_perp = np.array([v[1], -v[0]])\n",
    "    b = ((np.mean(data[labels==0,:],axis=0)+np.mean(data[labels==1,:],axis=0))/2)@v\n",
    "    lambda_vec = np.linspace(-2,2,len(data))\n",
    "    decision_boundary = b * v + lambda_vec * v_perp\n",
    "    return decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "\n",
    "N1 = 100 #number of points for class1\n",
    "N2 = 100 #number of points for class0\n",
    "covM = [1,0.01]*np.eye(2) # covariance matrix\n",
    "data = np.random.multivariate_normal([0,0], covM, N1) #generate points  for class 1\n",
    "X = np.vstack((data, np.random.multivariate_normal([1,1], covM, N2))) #generate points for class 0\n",
    "labels = np.hstack((np.ones(N1),np.zeros(N2)))\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=labels); plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, Y = fisherDiscriminant(X,labels)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1],c=labels)\n",
    "\n",
    "decision_boundary = discriminant(X, labels, v);\n",
    "plt.plot(decision_boundary[0,:], decision_boundary[1,:],'b',linewidth=2, label='Decision Boundary')\n",
    "plt.title(\"LDA\"); plt.axis('equal'); plt.legend(loc='best'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
