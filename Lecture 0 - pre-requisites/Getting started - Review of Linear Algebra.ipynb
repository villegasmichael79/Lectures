{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started: Review of Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Algebra is a big branch of mathematics concerning linear equations and their representations in vector spaces and through matrices. If you are watching this course, then you have already taken a Linear Algebra course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a brief review of concepts needed in the context of Data Science and Machine Learning. For a complete review, please follow the sources provided at the end of this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any concepts in this document that you are unfamiliar or *rusty* about, please review them as soon as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A [**vector**](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=2&t=0s) is the fundamental building block for Linear Algebra! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are many ways of interpreting a vector:\n",
    "* Ordered set of numbers (*data points*)\n",
    "* Arrows pointing in space, with some length and direction (*geometrical representation*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In Machine Learning and Data Science, a vector is often characterized as an ordered set of numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, we can have a $n$-dimensional vector that contains the number of positive COVID-19 cases in Florida in the last $n$ days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Vector Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some vector operations include:\n",
    "* Addition\n",
    "* Subtraction\n",
    "* Scalar multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consider the vectors, $\\mathbf{a}$ and $\\mathbf{b}$, and the scalar $c$.\n",
    "\n",
    "$$\\mathbf{a}=\\left[\\begin{array}{c}2\\\\1\\end{array}\\right] \\text{, } \\mathbf{b}=\\left[\\begin{array}{c}1\\\\-1\\end{array}\\right] \\text{ and } c= \\frac{1}{2}$$\n",
    "\n",
    "Let's look at these operations in the virtual whiteboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Inner product: $\\langle \\mathbf{a}, \\mathbf{b}\\rangle = \\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}^T \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\ldots + a_n b_n$\n",
    "* Outer product: $\\mathbf{a}\\otimes\\mathbf{b} = \\left[\\begin{array}{cccc}a_1 b_1 & a_1 b_2 & \\cdots & a_1\\\\ a_2 b_1 & a_2 b_2 & \\cdots & a_2 b_n \\\\ \\vdots & \\vdots &\\ddots & \\vdots \\\\a_n b_1 & a_n b_2 & \\cdots & a_n b_n \\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is *conventional* to write vectors as vertical vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Norms\n",
    "\n",
    "The **L-p norm** of a vector $\\mathbf{x}=\\left[x_1,x_2,\\cdots,x_n\\right]^T$ is:\n",
    "\n",
    "$$\\left\\Vert \\mathbf{x}\\right\\Vert_p = \\left(x_1^p + x_2^p + \\cdots + x_n^p\\right)^{1/p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Different $p$ norms have different (geometrical) properties. For example, the L-2 norm is commonly referred to as the **Euclidean norm**, and is denoted as $\\left\\Vert \\mathbf{x}\\right\\Vert_2$ or simply $\\left\\Vert \\mathbf{x}\\right\\Vert$.\n",
    "\n",
    "    * The L-2 norm computes the *length* of the vector: $\\text{length}(\\mathbf{x}) = \\Vert\\mathbf{x}\\Vert = \\sqrt{x_1^2 + \\cdots+x_n^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The L-2 norm relates to inner products by the **Cauchy-Schwarz inequality**:\n",
    "\n",
    "$$|\\mathbf{x}^T\\mathbf{y}|\\leq \\Vert\\mathbf{x}\\Vert \\Vert\\mathbf{y}\\Vert$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Within the Eulidean geometry, the **triangle inequality** is also a useful property that uses L-2 norm:\n",
    "\n",
    "$$\\Vert\\mathbf{x}+\\mathbf{y}\\Vert \\leq \\Vert\\mathbf{x}\\Vert + \\Vert\\mathbf{y}\\Vert$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distance\n",
    "    \n",
    "* The **Euclidean distance** between two nonzero vectors $\\mathbf{x}$ and $\\mathbf{y}$ is:\n",
    "\n",
    "$$d(\\mathbf{x},\\mathbf{y}) = \\Vert \\mathbf{x} - \\mathbf{y} \\Vert$$\n",
    "\n",
    "It measures the length of the *shortest* straight-line between two points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The cosine distance (formally **cosine similarity**) measures the angle between two vectors:\n",
    "\n",
    "$$d(\\mathbf{x},\\mathbf{y}) = \\frac{\\mathbf{x}^T \\mathbf{y}}{\\Vert \\mathbf{x}\\Vert \\Vert \\mathbf{y} \\Vert} = \\cos\\left(\\theta\\right)\\text{, where } \\theta=\\angle(\\mathbf{x},\\mathbf{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The **spherical distance** between two vectors $\\mathbf{x}$ and $\\mathbf{y}$ is given by $R\\times \\angle(\\mathbf{x},\\mathbf{y})$, where $R$ is the radius of the sphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unit vector\n",
    "\n",
    "In geometric representations, a vector is often characterized by its *direction* and *length*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When we are interested in only the direction of the vector, we usually *normalize* the vector by its L-2 norm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We say that a vector $\\mathbf{x}$ is a **unit vector** if $\\Vert \\mathbf{x}\\Vert=1$. \n",
    "\n",
    "* If $\\|\\mathbf{x} \\| \\ne 1$, we can create a unit vector in the same direction as $\\mathbf{x}$ as: $\\tilde{\\mathbf{x}} = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vector Correlation\n",
    "\n",
    "* The **vector (Pearson's) correlation** between $\\mathbf{x}$ and $\\mathbf{y}$ is\n",
    "\n",
    "$$r = \\frac{\\mathbf{x}^T\\mathbf{y}}{\\Vert\\mathbf{x}\\Vert \\Vert\\mathbf{y}\\Vert}$$\n",
    "\n",
    "(the same as cosine similarity!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vector Projection\n",
    "\n",
    "* The **projection** of $\\mathbf{y}$ onto $\\mathbf{x}$ is defined as:\n",
    "\n",
    "$$\\frac{\\mathbf{x}^T\\mathbf{y}}{\\Vert\\mathbf{x}\\Vert}$$\n",
    "\n",
    "* The **projection** of $\\mathbf{x}$ onto $\\mathbf{y}$ is defined as:\n",
    "\n",
    "$$\\frac{\\mathbf{x}^T\\mathbf{y}}{\\Vert\\mathbf{y}\\Vert}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Orthogonal and Orthonormal vectors\n",
    "\n",
    "* Two vectors $\\mathbf{x}$ and $\\mathbf{y}$ are **orthogonal** if their inner product is zero:\n",
    "\n",
    "$$\\mathbf{x}^T\\mathbf{y} = 0 \\Rightarrow \\mathbf{x} \\perp \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Two vectors are $\\mathbf{\\tilde{x}}$ and $\\mathbf{\\tilde{y}}$ are **orthonormal** if they are orthogonal and have unit norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We say that $\\{\\mathbf{\\tilde{x}},\\mathbf{\\tilde{y}}\\}$ is a set of orthonormal vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Span and Basis Vectors\n",
    "\n",
    "Consider, for example, the vectors $\\mathbf{x} = [1,0]$ and $\\mathbf{y}=[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The vector space $\\mathcal{S}=\\{\\mathbf{x},\\mathbf{y}\\}$ is a *spanning set* for $\\mathbb{R}^2$, or $\\mathcal{S}$ *spans* $\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Note that we can represent *any* vector in $\\mathbb{R}^2$ as a linear combination of the vectors $\\mathbf{x}$ and $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The **dimension** of a vector space $\\mathcal{S}$ is the cardinality (i.e. number of vectors) of a basis of $\\mathcal{S}$.\n",
    "\n",
    "    * A minimum of 2 spanning vectors are required to represent everything in $\\mathbb{R}^2$. So the dimension of $\\mathbb{R}^2$ is 2.\n",
    "    * Since the cardinality of $\\mathcal{S}$ is $|\\mathcal{S}|=2$, $\\mathcal{S}$ is *minimal*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We say that $\\mathbf{S}$ is a *minimal spanning set* or a **basis** of $\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Since the vectors in $\\mathbf{S}$ are orthonormal, we that $\\mathcal{S}$ is an orthonormal basis of $\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "Matrices are a generalization of vectors. One way to interpret matrices is: rectangular arrays or ordered numbers.\n",
    "\n",
    "Example: Suppose you are describing 3 houses in terms of their squared footage, average number of rooms and age.\n",
    "\n",
    "You can put this information in a matrix form: $\\left[\\begin{array}{cccc} 1214 & 4 & 65 \\\\ 2325 & 6 & 68\\\\ 1710 & 4 & 71 \\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Special Matrices\n",
    "\n",
    "* **Identity matrix**: $\\mathbf{I} = \\left[\\begin{array}{cccc}1 & 0 & \\cdots & 0\\\\ 0 & 1 &\\cdots&0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1\\end{array}\\right]$\n",
    "\n",
    "* **Diagonal matrix**: any matrix that can be written as the product of a constant with the identity matrix, $\\alpha \\mathbf{I} = \\left[\\begin{array}{cccc}\\alpha & 0 & \\cdots & 0\\\\ 0 & \\alpha &\\cdots&0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\alpha\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given the matrix $\\mathbf{A} = \\left[\\begin{array}{cc} 1 & 2\\\\ 3 & 4\\end{array}\\right]$, the vector $\\mathbf{x}=\\left[-1, 5\\right]^T$ and the scalar $d$:\n",
    "\n",
    "* Scalar-Matrix multiplication: $d \\mathbf{A} = \\mathbf{A} d = \\left[\\begin{array}{cc} d & 2d\\\\ 3d & 4d\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Vector-Matrix multiplication: $\\mathbf{A}\\mathbf{x} = \\left[\\begin{array}{cc} 1 & 2\\\\ 3 & 4\\end{array}\\right]\\left[\\begin{array}{c} -1 \\\\ 5\\end{array}\\right] = \\left[\\begin{array}{c} 1(-1) + 2(5) \\\\ 3(-1) + 4(5)\\end{array}\\right] = \\left[\\begin{array}{c} 9 \\\\ 17\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which operations are valid?: \n",
    "1. $\\mathbf{x}\\mathbf{A}$  <!-- **Not Valid, inner dimensions do not match** -->\n",
    "2. $\\mathbf{x}^T\\mathbf{A}$ <!-- Valid operation** -->\n",
    "3. $\\mathbf{x}^T\\mathbf{A}\\mathbf{x}$ <!-- **Valid operation** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Matrix-Matrix multiplication: $\\mathbf{A}\\mathbf{A} = \\left[\\begin{array}{cc} 1 & 2\\\\ 3 & 4\\end{array}\\right]\\left[\\begin{array}{cc} 1 & 2\\\\ 3 & 4\\end{array}\\right] = \\left[\\begin{array}{cc} 1(1) + 2(3) & 1(2)+2(4)\\\\ 3(1) + 4(3) & 3(2) + 4(4)\\end{array}\\right] = \\left[\\begin{array}{cc} 7 & 10\\\\ 15 & 22\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matrix Operators:\n",
    "\n",
    "* The **determinant** of a square $n\\times n$ matrix $\\mathbf{A}$ is a unique number. It measures the scaling factor by which the linear transformation $\\mathbf{A}$ changes any area or volume.\n",
    "\n",
    "The determinant of $\\mathbf{A}$ is denoted by $|\\mathbf{A}|$ or $\\det(\\mathbf{A})$. In Python, we can compute the determinant of a matrix using the module ```numpy.linalg```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Consider the vectors stacked vertically on the matrix $\\mathbf{A}$. If *any* of these vectors can be written as a linear combination of any other/s, then we say that $\\mathbf{A}$ has **linearly dependent columns**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If the matrix $\\mathbf{A}$ has *linearly dependent* columns (or rows) then $\\det(\\mathbf{A})=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* If the matrix $\\mathbf{A}$ has **linearly independent columns**, then $\\mathbf{A}$ is said to be **left-invertible**, that is there exists a matrix $\\mathbf{A}^{-1}$ such that $\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Similarly, if the rows of $\\mathbf{A}$ are linearly independent, then $\\mathbf{A}$ is **right-invertible**, that is there exists a matrix $\\mathbf{A}^{-1}$ such that $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The **trace** of a square $n\\times n$ matrix $\\mathbf{A}$ is defined as the sum of the elements on the main diagonal of $\\mathbf{A}$:\n",
    "\n",
    "$$\\text{trace}\\left(\\mathbf{A}\\right)= \\sum_{i=1}^n a_{ii}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Vector Function or *Affine Function*\n",
    "\n",
    "A linear matrix-vector multiplication function is defined as\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{f}: \\mathbb{R}^n &\\longrightarrow \\mathbb{R}^m \\\\\n",
    "\\mathbf{x} &\\longmapsto \\mathbf{A}\\mathbf{x}\n",
    "\\end{align}\n",
    "\n",
    "That is $\\mathbf{f}(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}$ where $\\mathbf{A}$ is a $m \\times n$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Since we are dealing with data, we generally start by assuming that the data is generated under some unknown **model** and then we pick a form for that model. For example, the linear vector function is a **linear model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Systems of Linear Equations\n",
    "\n",
    "* One of the most important applications of linear vector functions is to solve *linear systems of equations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The system of linear equations can be written concisely in matrix notation as\n",
    "$$ \\mathbf{Ax} = \\mathbf{b},$$\n",
    "where $\\mathbf{x}= [x_1, x_1,\\cdots, x_n]^T$ is the vector of variables (unknowns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* For example, consider the polynomial $p(\\mathbf{x}) = c_0 + c_1 \\mathbf{x} +c_2 \\mathbf{x}^2 + \\ldots c_p \\mathbf{x}^p$. We can write it in the form of $ \\mathbf{Ac} = \\mathbf{y}$, where\n",
    "\n",
    "$$\\mathbf{A} = \\left[\\begin{array}{ccccc}1 & x_1 & x_1^2 & \\cdots & x_1^p \\\\ 1 & x_2 & x_2^2 & \\cdots & x_2^p\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_n & x_n^2 & \\cdots & x_n^p\\end{array}\\right], \\mathbf{c}=\\left[\\begin{array}{c}c_0 \\\\ c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_p\\end{array}\\right] \\text{ and } \\mathbf{y} = p(\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where \n",
    "\n",
    "* $\\mathbf{A}$ is the $p^{th}$-order polynomial representation of the data points $\\mathbf{x}$ (e.g., house square footage, age and number of rooms)\n",
    "\n",
    "* $\\mathbf{y}$ is the output (e.g. house price)\n",
    "\n",
    "* $\\mathbf{c}=[c_0,c_1,c_2,\\cdots,c_p]^T$ are unknown coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Least Squares Solution\n",
    "\n",
    "If $\\mathbf{A}$ is invertible then\n",
    "\n",
    "$$\\mathbf{c} = \\mathbf{A}^{-1}\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Suppose that $p=1$ and $n>>p$, then $A = \\left[\\begin{array}{cc}1 & x_1\\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We only have 2 independent vectors, so they do not form a basis for $\\mathbb{R}^n$.\n",
    "\n",
    "In general, these problems do not have a solution, and we cannot use the matrix inverse to find the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Geometrically, you can write each row of $\\mathbf{A}$ as a line and you will find out that all $n$ lines will not intercept at the same point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **least squares solution** minimizes the sum of the squared errors:\n",
    "\n",
    "$$f(\\mathbf{c}) = \\Vert \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\Vert^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Then any minima $\\hat{\\mathbf{c}}$ will satisfy:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial c_i} (\\hat{\\mathbf{c}}) =0, ~~~ i=1,2,\\ldots,n$$\n",
    "\n",
    "Or using gradient notation\n",
    "$$ \\nabla f(\\hat{\\mathbf{c}}) =0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By taking the derivative of $f(\\mathbf{c})$ and solving for $\\mathbf{c}$, the **least squares solution** is then given by:\n",
    "\n",
    "$$\\mathbf{c} = \\left(\\mathbf{A}^T\\mathbf{A}\\right)^{-1}\\mathbf{A}^T\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where $\\mathbf{A}^{\\dagger} = \\left(\\mathbf{A}^T\\mathbf{A}\\right)^{-1}\\mathbf{A}^T$ is the **pseudo-inverse** (or **Moore–Penrose inverse**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "Every $n\\times n$ matrix is a *linear transformation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For each $n\\times n$ matrix, there are *special* vectors called **eigenvectors** that only get scaled by the linear transformation, that is, consider a vector $\\mathbf{v}$ that satisfies:\n",
    "\n",
    "$$\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}$$\n",
    "\n",
    "All vectors $\\mathbf{v}$ that satisfy this equation are called **eigenvectors** and the $\\lambda$ values are called **eigenvalues**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This equation is called the **characteristic equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If $\\mathbf{A}$ is an $n \\times n$ matrix with linearly independent rows, then $\\mathbf{A}$ has $n$ **orthogonal** eigenvectors, $\\mathbf{v}_1, \\mathbf{v}_2,\\ldots, \\mathbf{v}_{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* When we consider the normalized eigenvectors: $\\tilde{\\mathbf{v}}_i = \\frac{\\mathbf{v}_i}{\\Vert \\mathbf{v}_i\\Vert}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This means that the vector space $\\mathcal{S}=\\{\\tilde{\\mathbf{v}}_1,\\tilde{\\mathbf{v}}_2,\\cdots,\\tilde{\\mathbf{v}}_n\\}$ forms a **basis** of $\\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the characteristic equation and matrix multiplication, we can write:\n",
    "\n",
    "$$\\mathbf{V}^T\\mathbf{AV} = \\boldsymbol{\\Lambda}$$\n",
    "\n",
    "where $\\mathbf{V} = \\begin{bmatrix} \\mathbf{v}_0 & \\mathbf{v}_1 & \\cdots & \\mathbf{v}_{n-1} \\end{bmatrix}$ is the *modal matrix*, which has the eigenvectors as its columns, and\n",
    "\n",
    "$\\mathbf{\\Lambda}$ is a diagonal matrix that contains the associated eigenvalues, $\\mathbf{\\Lambda} = \\left[\\begin{array}{cc}\\lambda_1 & 0\\\\ 0 &\\lambda_2\\end{array}\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This is called the **Karhunen–Loève Transform (KLT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Further review\n",
    "\n",
    "If you are interested in reviewing these topics further, I recommend: \n",
    "\n",
    "* 3Blue1Brown, \"Essence of Linear Algebra\" YouTube series, [link](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n",
    "\n",
    "* Stephen Boyd and Lieven Vandenberghe, \"Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares\" book, [available online](http://vmls-book.stanford.edu/)\n",
    "\n",
    "* Gilbert Strang, 18.06 MITOpenCourseWare \"Linear Algebra\", [link](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
